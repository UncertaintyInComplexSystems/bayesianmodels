{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96SWkc20ciX8"
      },
      "source": [
        "# How to run basic GP regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKwBWZw9cj1T"
      },
      "outputs": [],
      "source": [
        "# !pip install git+ssh://git@github.com/UncertaintyInComplexSystems/bayesianmodels.git\n",
        "# !pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8JrgHPAFZff"
      },
      "source": [
        "Import the necessary stuff:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGK3SkpzntO1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# select CUDA device used by Jax\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = f'5'\n",
        "\n",
        "import jax\n",
        "import jax.random as jrnd\n",
        "import jax.numpy as jnp\n",
        "import distrax as dx\n",
        "import jaxkern as jk\n",
        "\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)  # crucial for Gaussian processes\n",
        "\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "from uicsmodels.gaussianprocesses.fullgp import FullLatentGPModel, FullMarginalGPModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAG6wB7bFZfh"
      },
      "source": [
        "Simulate some data from a known GP so we can look at the inference of the hyperparameters):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "AVM9NuejlN99",
        "outputId": "a54c1c6b-d39a-4919-a506-222567605d8f"
      },
      "outputs": [],
      "source": [
        "key = jrnd.PRNGKey(5609)  # 12345\n",
        "\n",
        "lengthscale_ = 1\n",
        "output_scale_ = 5.0\n",
        "obs_noise_ = 0.7\n",
        "n = 100\n",
        "x = jnp.linspace(0, 15, n)[:, jnp.newaxis]\n",
        "\n",
        "kernel = jk.RBF()\n",
        "K = kernel.cross_covariance(params=dict(lengthscale=lengthscale_,\n",
        "                                        variance=output_scale_),\n",
        "                            x=x, y=x) + 1e-6*jnp.eye(n)\n",
        "\n",
        "L = jnp.linalg.cholesky(K)\n",
        "z = jrnd.normal(key, shape=(n,))\n",
        "\n",
        "f_true = jnp.dot(L, z) + jnp.ones_like(z)\n",
        "key, obs_key = jrnd.split(key)\n",
        "y = f_true + obs_noise_*jrnd.normal(obs_key, shape=(n,))\n",
        "\n",
        "ground_truth = dict(f=f_true,\n",
        "                    lengthscale=lengthscale_,\n",
        "                    variance=output_scale_,\n",
        "                    obs_noise=obs_noise_)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(x, f_true, 'k', label=r'$f$')\n",
        "plt.plot(x, y, 'rx', label='obs')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "# plt.xlim([0., 1.])\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCzt_6wPFZfi"
      },
      "source": [
        "Set up the GP models, either with $\\mathbf{f}$ sampled explicitly, or with $\\mathbf{f}$ margnalized out (theobvious choice for a Gaussian likelihood, but sampling is shown for pedagogical reasons):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN4y4jQXwNLj"
      },
      "outputs": [],
      "source": [
        "priors = dict(kernel=dict(lengthscale=dx.Transformed(dx.Normal(loc=0.,\n",
        "                                                               scale=1.),\n",
        "                                                     tfb.Exp()),\n",
        "                          variance=dx.Transformed(dx.Normal(loc=0.,\n",
        "                                                            scale=1.),\n",
        "                                                  tfb.Exp())),\n",
        "              likelihood=dict(obs_noise=dx.Transformed(dx.Normal(loc=0.,\n",
        "                                                                 scale=1.),\n",
        "                                                       tfb.Exp())))\n",
        "\n",
        "gp_marginal = FullMarginalGPModel(x, y, cov_fn=jk.RBF(), priors=priors)  # Implies likelihood=Gaussian()\n",
        "gp_latent = FullLatentGPModel(x, y, cov_fn=jk.RBF(), priors=priors)  # Defaults to likelihood=Gaussian()\n",
        "\n",
        "num_particles = 1_00\n",
        "num_mcmc_steps = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A4Qz65kFZfk"
      },
      "source": [
        "Run inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgptAs6xK6M4",
        "outputId": "4de2f4e8-9a7f-41ab-c16d-2395dec5db55"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "%%script false --no-raise-error\n",
        "key, gpm_key = jrnd.split(key)\n",
        "with jax.disable_jit(disable=False), jax.debug_nans():\n",
        "    _, mgp_particles, _, mgp_marginal_likelihood = gp_marginal.inference(\n",
        "        gpm_key,\n",
        "        mode='gibbs-in-smc',\n",
        "        sampling_parameters=dict(\n",
        "            num_particles=num_particles, \n",
        "            num_mcmc_steps=num_mcmc_steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZIUP33-LDeD",
        "outputId": "f342335d-112a-4ebc-92ea-35af01c0339a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "with jax.disable_jit(disable=True), jax.debug_nans():\n",
        "    key, gpl_key = jrnd.split(key)\n",
        "    _, lgp_particles, _, lgp_marginal_likelihood = gp_latent.inference(\n",
        "        gpl_key,\n",
        "        mode='gibbs-in-smc',\n",
        "        sampling_parameters=dict(\n",
        "            num_particles=num_particles, \n",
        "            num_mcmc_steps=num_mcmc_steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7hxLP0SFZfl"
      },
      "source": [
        "Plot posteriors of hyperparameters for both models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "QWqkOFsdLG9m",
        "outputId": "27a42599-0a4a-4b97-a4e9-062ebc2b2690"
      },
      "outputs": [],
      "source": [
        "trainables = list()\n",
        "for component, val in priors.items():\n",
        "    trainables.extend(list(val.keys()))\n",
        "\n",
        "\n",
        "num_params = len(trainables)\n",
        "show_samples = jnp.array([int(i) for i in num_particles*jnp.linspace(0, 1, num=500)])\n",
        "\n",
        "symbols = dict(lengthscale='\\ell',\n",
        "               obs_noise='\\sigma',\n",
        "               variance=r'\\tau')\n",
        "\n",
        "_, axes = plt.subplots(nrows=2, ncols=num_params, constrained_layout=True,\n",
        "                       sharex='col', sharey='col', figsize=(12, 6))\n",
        "\n",
        "for m, particles in enumerate([mgp_particles, lgp_particles]):\n",
        "\n",
        "    unpacked_particles = {}  # NOTE: Prior structure changed but plotting was not updated. This is a quick fix, but no permanent solution.\n",
        "    unpacked_particles['lengthscale'] = particles.particles[\n",
        "        'kernel']['lengthscale']\n",
        "    unpacked_particles['variance'] = particles.particles[\n",
        "        'kernel']['lengthscale']\n",
        "    unpacked_particles['obs_noise'] = particles.particles[\n",
        "        'likelihood']['obs_noise']\n",
        "\n",
        "    for j, var in enumerate(trainables):\n",
        "        ax = axes[m, j]\n",
        "        pd = unpacked_particles[var]\n",
        "        # There are some outliers that skew the axis\n",
        "        pd_u, pd_l = jnp.percentile(pd, q=99.9), jnp.percentile(pd, q=0.1)\n",
        "        pd_filtered = jnp.extract(pd>pd_l, pd)\n",
        "        pd_filtered = jnp.extract(pd_filtered<pd_u, pd_filtered)\n",
        "        ax.hist(pd_filtered, bins=30, density=True, color='tab:blue')\n",
        "        if var in symbols and m==1:\n",
        "            ax.set_xlabel(r'${:s}$'.format(symbols[var]))\n",
        "\n",
        "plt.suptitle(f'Posterior estimate of Bayesian GP ({num_particles} particles)');\n",
        "\n",
        "axes[0, 0].set_ylabel('Marginal GP', rotation=0, ha='right')\n",
        "axes[1, 0].set_ylabel('Latent GP', rotation=0, ha='right')\n",
        "\n",
        "if len(ground_truth):\n",
        "    for j, var in enumerate(trainables):\n",
        "        axes[0, j].axvline(x=ground_truth[var], ls='--', c='k');\n",
        "        axes[1, j].axvline(x=ground_truth[var], ls='--', c='k');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYHPvfgvFZfn"
      },
      "source": [
        "And plot the posterior predictive of $\\mathbf{f}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "iNmqrrphJ-tX",
        "outputId": "66c13219-ebac-4a59-937b-85a6f4d86728"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "x_pred = jnp.linspace(-5, 20, num=150)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), sharex=True,\n",
        "                            sharey=True, constrained_layout=True)\n",
        "\n",
        "for j, gp in enumerate([gp_marginal, gp_latent]):\n",
        "    key, key_pred = jrnd.split(key)\n",
        "    f_pred = gp.predict_f(key_pred, x_pred)\n",
        "\n",
        "    ax = axes[j, 0]\n",
        "    for i in jnp.arange(0, num_particles, step=10):\n",
        "        ax.plot(x_pred, f_pred[i, :], alpha=0.1, color='tab:blue')\n",
        "\n",
        "    ax = axes[j, 1]\n",
        "    f_mean = jnp.mean(f_pred, axis=0)\n",
        "    f_hdi_lower = jnp.percentile(f_pred, q=2.5, axis=0)\n",
        "    f_hdi_upper = jnp.percentile(f_pred, q=97.5, axis=0)\n",
        "\n",
        "    ax.plot(x_pred, f_mean, color='tab:blue', lw=2)\n",
        "    ax.fill_between(x_pred, f_hdi_lower, f_hdi_upper,\n",
        "                    alpha=0.2, color='tab:blue', lw=0)\n",
        "\n",
        "for ax in axes.flatten():\n",
        "    ax.plot(x, f_true, 'k', label=r'$f$')\n",
        "    ax.plot(x, y, 'rx', label='obs')\n",
        "    #ax.set_xlim([-0.25, 1.25])\n",
        "    #ax.set_ylim([-4., 5.])\n",
        "    ax.set_xlabel(r'$x$')\n",
        "\n",
        "axes[0, 0].set_title('SMC particles')\n",
        "axes[0, 1].set_title('Posterior 95% HDI')\n",
        "\n",
        "axes[0, 0].set_ylabel('Marginal GP', rotation=0, ha='right')\n",
        "axes[1, 0].set_ylabel('Latent GP', rotation=0, ha='right');"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
