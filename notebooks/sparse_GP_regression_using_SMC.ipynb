{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96SWkc20ciX8"
      },
      "source": [
        "# Sparse GP regression with Gibbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PKwBWZw9cj1T"
      },
      "outputs": [],
      "source": [
        "# !pip install git+ssh://git@github.com/UncertaintyInComplexSystems/bayesianmodels.git\n",
        "# !pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8JrgHPAFZff"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VGK3SkpzntO1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# select CUDA device used by Jax\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = f'5'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "plt.rc('axes', titlesize=18)        # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=16)        # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=14)       # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=14)       # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=16)       # legend fontsize\n",
        "plt.rc('figure', titlesize=20)      # fontsize of the figure title\n",
        "plt.style.use('Solarize_Light2')\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "from icecream import ic\n",
        "ic.configureOutput(includeContext=True)\n",
        "\n",
        "import jax\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)  # crucial for Gaussian processes\n",
        "import jax.random as jrnd\n",
        "import jax.numpy as jnp\n",
        "import distrax as dx\n",
        "from distrax._src.distributions.distribution import Distribution\n",
        "from distrax._src.bijectors.bijector import Bijector\n",
        "import jaxkern as jk\n",
        "from jax.tree_util import tree_flatten\n",
        "\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "from uicsmodels.gaussianprocesses.sparsegp import SparseGPModel\n",
        "from uicsmodels.gaussianprocesses.fullgp import FullLatentGPModel, FullMarginalGPModel\n",
        "\n",
        "# confirm precision setting\n",
        "x = jrnd.uniform(jrnd.PRNGKey(0), (1000,), dtype=jnp.float64)\n",
        "x.dtype # --> dtype('float64')\n",
        "\n",
        "print('hello cruel world')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cov(cov, title, iter='', show_negative=True, figsize=(4, 12)):    \n",
        "    # transform jax array to numpy array to allow mutation\n",
        "    cov_np = np.array(cov)  \n",
        "\n",
        "    cmap = plt.cm.plasma\n",
        "\n",
        "    # replace negativ values with nan to make them visible in the plot\n",
        "    if show_negative:\n",
        "        cmap.set_bad((.5, 0, 0, 1))\n",
        "        for c in np.argwhere(cov_np < 0):\n",
        "            cov_np[c[0], c[1]] = np.nan\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize) \n",
        "    image = plt.imshow(cov_np, cmap=cmap) \n",
        "    # creating new axes on the right side of (ax) for the colorbar\n",
        "    # this new axis has the same hight as the original axis\n",
        "    divider = make_axes_locatable(ax) \n",
        "    colorbar_axes = divider.append_axes(\"right\", \n",
        "                                        size=\"10%\", \n",
        "                                        pad=0.1)\n",
        "    plt.colorbar(image, \n",
        "                 cax=colorbar_axes, \n",
        "                 ticks=None if show_negative else [np.min(cov_np), np.mean(cov_np), np.max(cov_np)])\n",
        "\n",
        "    # colorbar_axes.ticklabel_format(style='plain')\n",
        "    ax.set_title(title)\n",
        "    plt.savefig(f'results/cov_{title}_iter{iter}.png')\n",
        "    plt.savefig(f'results/cov_{title}.png')\n",
        "    # plt.show()\n",
        "    plt.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAG6wB7bFZfh"
      },
      "source": [
        "## simulate data\n",
        "Simulate some data from a known GP so we can look at the inference of the hyperparameters):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "AVM9NuejlN99",
        "outputId": "a54c1c6b-d39a-4919-a506-222567605d8f"
      },
      "outputs": [],
      "source": [
        "random_random_seed = np.random.randint(0, 10000 + 1)\n",
        "print('seed:', random_random_seed)\n",
        "key_data = jrnd.PRNGKey(12345)  # 1106, 5368, 8928, 5609\n",
        "\n",
        "lengthscale_ = 0.2\n",
        "output_scale_ = 5.0\n",
        "obs_noise_ = 0.7\n",
        "n = 100\n",
        "x = jnp.linspace(0, 1, n)[:, jnp.newaxis]  # NOTE: Rossie seem to use x-domain of [-1, 1]\n",
        "\n",
        "x_mean = jnp.mean(x)\n",
        "x_std = jnp.std(x)\n",
        "print(f'x: mean {jnp.round(x_mean, decimals=10)}, std {x_std}')\n",
        "\n",
        "kernel = jk.RBF()\n",
        "K = kernel.cross_covariance(params=dict(lengthscale=lengthscale_,\n",
        "                                        variance=output_scale_),\n",
        "                            x=x, y=x) + 1e-6*jnp.eye(n)\n",
        "# plot_cov(K, 'true cov')\n",
        "\n",
        "L = jnp.linalg.cholesky(K)\n",
        "z = jrnd.normal(key_data, shape=(n,))\n",
        "f_true = jnp.dot(L, z) + jnp.zeros_like(z)  # NOTE: True GP had mean=1\n",
        "\n",
        "_, obs_key = jrnd.split(key_data)\n",
        "y = f_true + obs_noise_*jrnd.normal(obs_key, shape=(n,))\n",
        "\n",
        "ground_truth = dict(\n",
        "    f=f_true,\n",
        "    lengthscale=lengthscale_,\n",
        "    variance=output_scale_,\n",
        "    obs_noise=obs_noise_)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(x, f_true, 'k', label=r'$f$')\n",
        "plt.plot(x, y, 'rx', label='obs')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "# plt.xlim([0., 1.])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WIP implementing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCzt_6wPFZfi"
      },
      "source": [
        "Set up the GP models, either with $\\mathbf{f}$ sampled explicitly, or with $\\mathbf{f}$ margnalized out (theobvious choice for a Gaussian likelihood, but sampling is shown for pedagogical reasons):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prior & init. model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prior over covariance hyperparameters, likelihood, and inducing inputs.\n",
        "\n",
        "num_inducing_points = 20\n",
        "\n",
        "priors = dict(\n",
        "    kernel=dict(\n",
        "        lengthscale = dx.Transformed( \n",
        "            dx.Normal(loc=0.0, scale=1.0),\n",
        "            tfb.Exp()), \n",
        "        #lengthscale = dx.Deterministic(loc=lengthscale_),\n",
        "\n",
        "        variance = dx.Transformed( \n",
        "            dx.Normal(loc=0.0, scale=1.0),\n",
        "            tfb.Exp())),\n",
        "        #variance = dx.Deterministic(loc=output_scale_)),\n",
        "\n",
        "    likelihood=dict(  # actually not modeled in Rossi\n",
        "        obs_noise = dx.Transformed(\n",
        "            dx.Normal(loc=0.0, scale=1.0), \n",
        "            tfb.Exp())),\n",
        "        #obs_noise = dx.Deterministic(loc=obs_noise_)),\n",
        "        \n",
        "    inducing_inputs_Z=dict( \n",
        "    # NOTE: Defining distribution over Z directly results in an error\n",
        "        # the __init__ tries to sample from each distribution in the prior.\n",
        "        mean=dx.Deterministic(\n",
        "            loc=jnp.ones(shape=num_inducing_points) * x_mean),\n",
        "        scale=dx.Deterministic(\n",
        "            loc=jnp.ones(shape=num_inducing_points) * x_std*.8)))\n",
        "\n",
        "gp_sparse = SparseGPModel(\n",
        "    x, y, \n",
        "    cov_fn=jk.RBF(), priors=priors, \n",
        "    num_inducing_points=num_inducing_points,\n",
        "    f_true=f_true)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibbs (for-loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### first gibbs_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%script false --no-raise-error  # hack to skip execution of a cell\n",
        "\n",
        "# playground, for testing only\n",
        "gibbs_state = gp_sparse.init_fn(\n",
        "    key=key, \n",
        "    num_particles = sampling_parameters['num_particles'])\n",
        "\n",
        "def plot_prior():\n",
        "    # plot true function, inducing points, estimated function\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5)\n",
        "    plt.plot(x, gibbs_state.position['f'], label=r'prior $f$', color='lightblue')\n",
        "    plt.plot(\n",
        "        gibbs_state.position['Z'], \n",
        "        gibbs_state.position['u'], \n",
        "        'x', \n",
        "        label='inducing points', \n",
        "        color='red',\n",
        "        alpha=1.)\n",
        "    plt.xlabel('x & Z')\n",
        "    plt.ylabel('y & u')\n",
        "    # plt.xlim([0., 1.])\n",
        "    plt.legend()\n",
        "    plt.savefig(f'results/prior.png')\n",
        "    plt.show()\n",
        "\n",
        "plot_prior()\n",
        "\n",
        "# plot covariance matrices\n",
        "def _plot_covs():\n",
        "    for t in gibbs_state.position['dev'].keys():\n",
        "        c = gibbs_state.position['dev'][t]\n",
        "        print(f'{t}: {c.shape}')\n",
        "        if t == 'cov_gp':\n",
        "            plot_cov(c, t, show_negative=False, figsize=(8, 12))\n",
        "            plot_cov(c, t + '\\nred = negative values', \n",
        "                    show_negative=True, \n",
        "                    figsize=(8, 12))\n",
        "        else:\n",
        "            plot_cov(c, t, show_negative=False)\n",
        "\n",
        "# _plot_covs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# utils: pretty print gibbs state, IPython progress bar\n",
        "\n",
        "def update_progress(progress, extra=''):\n",
        "    bar_length = 50\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "\n",
        "    block = int(round(bar_length * progress))\n",
        "    clear_output(wait = True)\n",
        "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
        "    print(text, extra)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "%%script false --no-raise-error  # hack to skip execution of a cell\n",
        "\n",
        "gibbs_state_hist = [gibbs_state]\n",
        "for i in range(1, sampling_parameters['num_mcmc_steps']):\n",
        "    t_start = time.time()\n",
        "\n",
        "    key, key_gibbs = jrnd.split(key)  # TODO: Should I reuse the same key every iteration instead of splitting a new one?\n",
        "    gibbs_state_, _ = gp_sparse.gibbs_fn(key_gibbs, gibbs_state_hist[-1], sampling_parameters)\n",
        "    gibbs_state_hist.append(gibbs_state_)\n",
        "    \n",
        "    # dumb current gibbst state history\n",
        "    if i % 100 == 0:\n",
        "        file = open(f'results/gibbs_state_hist.pkl', 'wb')\n",
        "        pickle.dump(gibbs_state_hist, file)\n",
        "        file.close()\n",
        "\n",
        "    # update progressbar\n",
        "    if i % 10 == 0:\n",
        "        update_progress(i / sampling_parameters[\"num_mcmc_steps\"])\n",
        "\n",
        "    # show and save plots\n",
        "    if (i % 50 == 0) and (i > 0):\n",
        "        plot_posterior(gibbs_state_hist, num_of_samples=1_000, iter=i)\n",
        "        plot_inducing_variables(gibbs_state_hist)\n",
        "        plot_training_history(gibbs_state_hist)\n",
        "        # plot_cov(gibbs_state_hist[-1].position['dev']['cov_gp'], title='gp', iter=i)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f'    iter {i}, duration: {jnp.round(time.time() - t_start, decimals = 3)} sec')\n",
        "\n",
        "update_progress(1)\n",
        "plot_posterior(\n",
        "    gibbs_state_hist,   \n",
        "    num_of_samples=1_000, \n",
        "    iter=sampling_parameters['num_mcmc_steps'])\n",
        "plot_inducing_variables(gibbs_state_hist)\n",
        "plot_training_history(gibbs_state_hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibbs (Jax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%script false --no-raise-error  # hack to skip execution of a cell\n",
        "%%time\n",
        "\n",
        "# randomize the models inital state\n",
        "random_random_seed = np.random.randint(0, 10000 + 1)\n",
        "print('random random seed:', random_random_seed)\n",
        "key = jrnd.PRNGKey(random_random_seed)\n",
        "\n",
        "# define parameters for inference\n",
        "sampling_parameters = dict(\n",
        "    num_burn=0,  # gibbs\n",
        "    num_samples=20_000  # gibbs | num_mcmc_steps\n",
        "    )\n",
        "    \n",
        "# run inference\n",
        "with jax.disable_jit(disable=False):  # TODO: Add alarming print when disabled\n",
        "    key, key_inference = jrnd.split(key)\n",
        "\n",
        "    initial_state, gibbs_states = gp_sparse.inference(\n",
        "        key_inference, \n",
        "        mode='gibbs', \n",
        "        sampling_parameters=sampling_parameters)\n",
        "\n",
        " # save all data to file\n",
        "if False:\n",
        "    data = {\n",
        "        'x': x,\n",
        "        'f_true': f_true,\n",
        "        'initial_state': initial_state,\n",
        "        'gibbs_states': gibbs_states\n",
        "    }\n",
        "    file = open(f'results/gibbs_states.pkl', 'wb')\n",
        "    pickle.dump(data, file)\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "print('\\nGibbs-state keys:')\n",
        "# print all keys and len/shape of corresponding data\n",
        "for k in gibbs_states.position.keys():\n",
        "    try:\n",
        "        print('    ', k, \n",
        "              gibbs_states.position[k]\n",
        "              if len(gibbs_states.position[k].shape) == 0 \n",
        "              else gibbs_states.position[k].shape, \n",
        "              end='\\n')\n",
        "    except:\n",
        "        print('    ', k, len(gibbs_states.position[k]), end='\\n')\n",
        "print()\n",
        "\n",
        "print('\\ninital values')\n",
        "print('lengthscale:', initial_state.position['lengthscale'])\n",
        "print('variance', initial_state.position['variance'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "def plot_prior(initial_state):\n",
        "    \"\"\"\n",
        "    Plot true function and prior\n",
        "    \n",
        "    num_of_states - number of samples starting from the back of the chain to use for plotting the posterior. \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5, zorder=-1)\n",
        "\n",
        "    # prior\n",
        "    plt.plot(\n",
        "        x, \n",
        "        initial_state.position['f'], \n",
        "        label=r'prior $f$', \n",
        "        color='lightblue',\n",
        "        alpha=1,\n",
        "        lw=2,\n",
        "        zorder=1)\n",
        "    plt.plot(\n",
        "        initial_state.position['Z'], \n",
        "        initial_state.position['u'], \n",
        "        'x', \n",
        "        label='prior\\ninducing points', \n",
        "        color='cornflowerblue',\n",
        "        alpha=1,\n",
        "        lw=1,\n",
        "        zorder=1)\n",
        "    \n",
        "    plt.title(f'true function, noisy data, and prior')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_posterior(gibbs_state_hist, initial_state, num_of_samples=100):\n",
        "    \"\"\"\n",
        "    Plot true function and posterior samples\n",
        "    \n",
        "    num_of_states - number of samples starting from the back of the chain to use for plotting the posterior. \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5, zorder=-1)\n",
        "\n",
        "    plt.plot(\n",
        "        gibbs_state_hist.position['Z'][-1],\n",
        "        # jnp.mean(gibbs_state_hist.position['Z'][-num_of_samples: ], axis=0),\n",
        "        gibbs_state_hist.position['u'][-1], \n",
        "        # jnp.mean(gibbs_state_hist.position['u'][-num_of_samples: ], axis=0),\n",
        "        'x', \n",
        "        label='posterior\\ninducing points\\nlast sample', \n",
        "        color='red',\n",
        "        alpha=1.0,\n",
        "        zorder=2)\n",
        "\n",
        "    # plt.plot(\n",
        "    #     gibbs_state_hist.position['Z'][-num_of_samples: ],\n",
        "    #     gibbs_state_hist.position['u'][-num_of_samples: ], \n",
        "    #     'x', \n",
        "    #     # label='posterior\\ninducing points\\nall samples', \n",
        "    #     color='red',\n",
        "    #     alpha=0.1,\n",
        "    #     zorder=0)\n",
        "\n",
        "    plt.plot(\n",
        "        # gibbs_state_hist.position['Z'][-1],\n",
        "        jnp.mean(gibbs_state_hist.position['Z'][-num_of_samples: ], axis=0),\n",
        "        #gibbs_state_hist.position['u'][-1], \n",
        "        jnp.mean(gibbs_state_hist.position['u'][-num_of_samples: ], axis=0),\n",
        "        'x', \n",
        "        label='mean posterior\\ninducing points', \n",
        "        color='orange',\n",
        "        alpha=1.0,\n",
        "        zorder=2)\n",
        "\n",
        "    # prior\n",
        "    plt.plot(\n",
        "        x, \n",
        "        initial_state.position['f'], \n",
        "        label=r'prior $f$', \n",
        "        color='lightblue',\n",
        "        alpha=1,\n",
        "        lw=2,\n",
        "        zorder=1)\n",
        "    plt.plot(\n",
        "        initial_state.position['Z'], \n",
        "        initial_state.position['u'], \n",
        "        'x', \n",
        "        label='prior\\ninducing points', \n",
        "        color='cornflowerblue',\n",
        "        alpha=1,\n",
        "        lw=1,\n",
        "        zorder=1)\n",
        "\n",
        "    # get last `num_of_states` gibbs states\n",
        "    f_samples = gibbs_state_hist.position['f'][-1*num_of_samples:]\n",
        "\n",
        "    # plot each sample\n",
        "    '''for i in range(f_samples.shape[0]):  # negativ indexing to select last elements\n",
        "        plt.plot(x, f_samples[i], \n",
        "                color='orange',\n",
        "                alpha=0.1)'''\n",
        "        \n",
        "    # plot average and confidence intervals\n",
        "    f_mean = jnp.mean(f_samples, axis=0)\n",
        "    f_hdi_lower = jnp.percentile(f_samples, q=2.5, axis=0)\n",
        "    f_hdi_upper = jnp.percentile(f_samples, q=97.5, axis=0)\n",
        "    ax = plt.gca()\n",
        "    ax.plot(x, f_mean, label='mean f~GP', color='green', lw=1.5, zorder=1)\n",
        "    ax.fill_between(\n",
        "        x.flatten(), f_hdi_lower, f_hdi_upper, \n",
        "        alpha=0.5, color='lightgreen', lw=0)\n",
        "    # ax.set_ylim(\n",
        "    #     jnp.min(f_true) - 1, \n",
        "    #     jnp.max(f_true) + 1)\n",
        "    \n",
        "    plt.title(f'last {f_samples.shape[0]} posterior samples')\n",
        "    plt.legend()\n",
        "    # plt.savefig(f'results/posterior_iter{iter}.png')\n",
        "    # plt.savefig(f'results/posterior.png')\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "    \n",
        "def plot_cov_param_trajectory(lengthscale=None, variance=None, title=''):\n",
        "    # Plot training history\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax2 = ax.twinx()\n",
        "    cmap = plt.colormaps.get_cmap('Set1')\n",
        "\n",
        "    if lengthscale is not None:\n",
        "        ax.plot(\n",
        "            lengthscale, \n",
        "            label='lengthscale', \n",
        "            color=cmap(0), alpha=1,\n",
        "            zorder=1)\n",
        "        ax.axhline(\n",
        "            lengthscale_, \n",
        "            linestyle='--', \n",
        "            label=f'true lengthscale ({lengthscale_})',\n",
        "            color=cmap(0), alpha=1,\n",
        "            zorder=2)\n",
        "        ax.set_ylabel('lengthscale', color=cmap(0))\n",
        "        lengthscale_mean = jnp.mean(lengthscale)\n",
        "        lengthscale_hdi_lower = jnp.percentile(lengthscale, q=2.5, axis=0)\n",
        "        lengthscale_hdi_upper = jnp.percentile(lengthscale, q=97.5, axis=0)\n",
        "        # ax.set_ylim(lengthscale_hdi_lower*1.5, lengthscale_hdi_upper*1.5)\n",
        "        # if lengthscale.shape[0] >= 200:\n",
        "        #     ax.set_ylim(\n",
        "        #         jnp.min(lengthscale[400:])*1.5, \n",
        "        #         jnp.max(lengthscale[400:])*1.5)\n",
        "\n",
        "    if variance is not None:\n",
        "        ax2.plot(\n",
        "            variance, \n",
        "            label='variance', \n",
        "            color=cmap(1), alpha=1,\n",
        "            zorder=3)\n",
        "        ax2.axhline(output_scale_, \n",
        "                    linestyle='--', \n",
        "                    label=f'true variance ({output_scale_})',\n",
        "                    color=cmap(1), alpha=1,\n",
        "                    zorder=4)\n",
        "        ax2.set_ylabel('variance', color=cmap(1))\n",
        "\n",
        "    # create shared legend for both axis\n",
        "    lines, labels = ax.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax2.legend(lines + lines2, labels + labels2, loc=0)\n",
        "\n",
        "    ax.set_xlabel('Gibbs iterations')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "\n",
        "def plot_param_trajectory(\n",
        "        param_gibbs_hist:dict, \n",
        "        title:str='', \n",
        "        color_idxs:list=[0, 1]):\n",
        "    # Plot training history\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))    \n",
        "    cmap = plt.colormaps.get_cmap('Set1')\n",
        "\n",
        "    for i, (param, data) in enumerate(param_gibbs_hist.items()):\n",
        "\n",
        "        if i == 1:\n",
        "            ax = ax.twinx()\n",
        "        ax.plot(data, label=param, color=cmap(color_idxs[i]))\n",
        "        # ax.axhline(lengthscale_, linestyle='--', color=cmap(0), \n",
        "        #         label=f'true lengthscale ({lengthscale_})')\n",
        "        ax.set_ylabel(param, color=cmap(color_idxs[i]))\n",
        "        ax.legend()\n",
        "\n",
        "    # create shared legend for both axis\n",
        "    # lines, labels = ax.get_legend_handles_labels()\n",
        "    #lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    # ax2.legend(lines + lines2, labels + labels2, loc=0)\n",
        "\n",
        "    ax.set_xlabel('iterations')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "\n",
        "def plot_u_trajectories(gibbs_state_hist, gibbs_key:str='u'):\n",
        "    # plot u trajectories\n",
        "    u_samples = gibbs_state_hist.position[gibbs_key]\n",
        "\n",
        "    iter_idx = range(len(u_samples))\n",
        "    color = plt.cm.hsv(np.linspace(0, 1, u_samples.shape[1]))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    ax = plt.gca()\n",
        "    ax.set_facecolor('grey')\n",
        "    for i in range(u_samples.shape[1]):\n",
        "        plt.plot(iter_idx, u_samples[:, i], alpha=0.7, color=color[i], lw=1)\n",
        "    plt.xlabel('Gibbs iterations')\n",
        "    plt.title(f'{gibbs_key} samples')\n",
        "    # plt.savefig(f'results/train_hist_{gibbs_key}.png')\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "\n",
        "def plot_inducing_variables(gibbs_state_hist):\n",
        "    plot_u_trajectories(gibbs_state_hist, gibbs_key='u')\n",
        "    plot_u_trajectories(gibbs_state_hist, gibbs_key='Z')\n",
        "\n",
        "def plot_param_histogram(\n",
        "    param_samples:dict,\n",
        "    bins:int = 20,\n",
        "    title:str='', \n",
        "    color_idxs:list=[0, 1]):\n",
        "\n",
        "    fig, ax = plt.subplots(\n",
        "        ncols=len(param_samples), \n",
        "        figsize=(12, 4), \n",
        "        squeeze=False)\n",
        "    cmap = plt.colormaps.get_cmap('Set1')\n",
        "\n",
        "    for i, (param, data) in enumerate(param_samples.items()):\n",
        "        ax[0, i].hist(\n",
        "            data,\n",
        "            density=True,\n",
        "            bins=bins, \n",
        "            color=cmap(color_idxs[i]),)\n",
        "        ax[0, i].set_title(param)\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_prior(initial_state)\n",
        "plot_posterior(\n",
        "    gibbs_states,\n",
        "    initial_state,\n",
        "    num_of_samples=sampling_parameters['num_samples'])\n",
        "plot_param_histogram(\n",
        "    param_samples = {\n",
        "        'lengthscale': gibbs_states.position['lengthscale'],\n",
        "        'variance': gibbs_states.position['variance'],\n",
        "        'obs_noise': gibbs_states.position['obs_noise']\n",
        "        },\n",
        "    color_idxs=[0, 1, 2]\n",
        ")\n",
        "plot_inducing_variables(gibbs_states)\n",
        "plot_cov_param_trajectory(\n",
        "    lengthscale = gibbs_states.position['lengthscale'],\n",
        "    variance = gibbs_states.position['variance'])\n",
        "plot_cov_param_trajectory(\n",
        "    lengthscale = gibbs_states.position['lengthscale'])\n",
        "plot_cov_param_trajectory(\n",
        "    variance = gibbs_states.position['variance'])\n",
        "\n",
        "plot_cov_param_trajectory(\n",
        "    lengthscale = gibbs_states.position['lengthscale'][:100],\n",
        "    variance = gibbs_states.position['variance'][:100],\n",
        "    title='first 100 iterations')\n",
        "plot_param_trajectory(\n",
        "    param_gibbs_hist = {\n",
        "        'lengthscale': gibbs_states.position['lengthscale'][200:],\n",
        "        'variance': gibbs_states.position['variance'][200:],\n",
        "        },\n",
        "    title='after 200 iter',\n",
        "    color_idxs=[0, 1]\n",
        "    )\n",
        "plot_param_trajectory(\n",
        "    param_gibbs_hist = {'obs_noise': gibbs_states.position['obs_noise']},\n",
        "    color_idxs=[2]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SMC-in-Gibbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# TODO: Move everything but infernece call to different cell for accurate run-time measurement\n",
        "\n",
        "# randomize the models inital state\n",
        "random_random_seed = np.random.randint(0, 10000 + 1)\n",
        "# random_random_seed = 8180\n",
        "print('random random seed:', random_random_seed)\n",
        "key = jrnd.PRNGKey(random_random_seed)\n",
        "\n",
        "# define parameters for inference\n",
        "sampling_parameters = dict(\n",
        "    num_particles = 100,  # SMC\n",
        "    num_mcmc_steps = 10   # SMC\n",
        "    )\n",
        "    \n",
        "# run inference\n",
        "with jax.disable_jit(disable=False), jax.debug_nans():\n",
        "    # TODO: Add alarming print when disabled=True\n",
        "    # BUG: JAX does not detect nan's when jit disable=False\n",
        "    key, key_inference = jrnd.split(key)\n",
        "\n",
        "    initial_particles, particles, _, marginal_likelihood = gp_sparse.inference(\n",
        "        key_inference, \n",
        "        mode='gibbs-in-smc', \n",
        "        sampling_parameters=sampling_parameters)\n",
        "\n",
        "    # unpack results for easy access\n",
        "    particles = particles.particles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "def f(x):\n",
        "  return jnp.log(x) * jnp.nan\n",
        "\n",
        "r = 0\n",
        "with jax.disable_jit(disable=False), jax.debug_nans():\n",
        "    r = jax.jit(f)(1e-400)\n",
        "\n",
        "print(r)\n",
        "\n",
        "'''\n",
        "Here debug_nans is triggered if code is jitted. \n",
        "meaning the GP code produces NaN's when not jitted?\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('prior samples means\\n')\n",
        "for k in initial_particles.position.keys():\n",
        "    curr_part = initial_particles.position[k]\n",
        "    \n",
        "    print(k, end=':\\n')\n",
        "    if type(curr_part) == dict:\n",
        "        for sk in curr_part.keys():\n",
        "            curr_hyp = curr_part[sk]\n",
        "            print(\n",
        "                '    ', \n",
        "                sk, curr_hyp.shape, \n",
        "                jnp.round(jnp.mean(curr_hyp), decimals=3), \n",
        "                jnp.round(jnp.var(curr_hyp), decimals=3))\n",
        "    else:\n",
        "        print(\n",
        "            '  ',\n",
        "            curr_part.shape)\n",
        "\n",
        "prior_lengthscale_mean = jnp.mean(initial_particles.position['kernel']['lengthscale'])\n",
        "prior_variance_mean = jnp.mean(initial_particles.position['kernel']['variance'])\n",
        "prior_obs_noise_mean = jnp.mean(initial_particles.position['likelihood']['obs_noise'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full latent GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# lgp_inital = initial_particles\n",
        "# lgp_particles = particles\n",
        "# lgp_marginal_likelihood = marginal_likelihood\n",
        "# %%script false --no-raise-error\n",
        "# TODO: Move everything but infernece call to different cell for accurate run-time measurement\n",
        "\n",
        "# get subset of priors used for sparse GP\n",
        "priors_lgp = {}\n",
        "priors_lgp['kernel'] = priors['kernel']\n",
        "priors_lgp['likelihood'] = priors['likelihood']\n",
        "\n",
        "gp_latent = FullLatentGPModel(x, y, cov_fn=jk.RBF(), priors=priors_lgp)\n",
        "\n",
        "key, gpl_key = jrnd.split(jrnd.PRNGKey(random_random_seed))\n",
        "lgp_inital, lgp_particles, _, lgp_marginal_likelihood = gp_latent.inference(\n",
        "    gpl_key,\n",
        "    mode='gibbs-in-smc',\n",
        "    sampling_parameters=sampling_parameters)\n",
        "\n",
        "lgp_particles = lgp_particles.particles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Posterior(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(marginal_likelihood, jnp.exp(marginal_likelihood))  # BUG: marginal likelihood of sparse GP is nan\n",
        "print(lgp_marginal_likelihood, jnp.exp(lgp_marginal_likelihood))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def particle_avg_hdi(particles):\n",
        "    # plot average and confidence intervals\n",
        "    mean = jnp.mean(particles, axis=0)\n",
        "    hdi_lower = jnp.percentile(particles, q=2.5, axis=0)\n",
        "    hdi_upper = jnp.percentile(particles, q=97.5, axis=0)\n",
        "    return mean, hdi_lower, hdi_upper\n",
        "\n",
        "\n",
        "def plot_smc_prior(initial_particles, title='', figsize=(12, 4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # observations and true function\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "\n",
        "    # detect Nans and print locations\n",
        "    nans = np.argwhere(np.isnan(initial_particles.position['f']))\n",
        "    for p in np.unique(nans[:, 0]):\n",
        "        nan_loc = nans[np.argwhere(nans[:, 0] == p)][:, 0, 1]\n",
        "        ic(f'{len(nan_loc)} NaNs in f in particle {p}')\n",
        "\n",
        "     # prior f\n",
        "    f_prior_mean, _, _  = particle_avg_hdi(\n",
        "        initial_particles.position['f'])\n",
        "    plt.plot(\n",
        "        x, \n",
        "        f_prior_mean, \n",
        "        label=r'prior $f$', \n",
        "        color='lightblue',\n",
        "        alpha=1,\n",
        "        lw=2,\n",
        "        zorder=1)\n",
        "    \n",
        "    # prior inducing points\n",
        "    if 'Z' in initial_particles.position.keys():\n",
        "        (Z_prior_mean, \n",
        "        Z_prior_hdi_lower, \n",
        "        Z_prior_hdi_upper) = particle_avg_hdi(initial_particles.position['Z'])\n",
        "        (u_prior_mean, \n",
        "        u_prior_hdi_lower, \n",
        "        u_prior_hdi_upper) = particle_avg_hdi(initial_particles.position['u'])\n",
        "        plt.plot(\n",
        "            Z_prior_mean, \n",
        "            u_prior_mean, \n",
        "            'x', \n",
        "            label='prior\\ninducing points', \n",
        "            color='cornflowerblue',\n",
        "            alpha=1,\n",
        "            lw=1,\n",
        "            zorder=1)\n",
        "        \n",
        "    plt.legend()\n",
        "    plt.title(title, fontsize=15)\n",
        "    plt.show()\n",
        "\n",
        "def plot_smc_posterior(particles, title='', figsize=(12, 4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # observations and true function\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5, zorder=-1)\n",
        "\n",
        "    # f | particle average\n",
        "    fp_mean, fp_hdi_lower, fp_hdi_upper = particle_avg_hdi(particles['f'])\n",
        "    ax = plt.gca()\n",
        "    ax.plot(x, fp_mean, label='posterior f', color='green', lw=1.5, zorder=1)\n",
        "    ax.fill_between(\n",
        "        x.flatten(), fp_hdi_lower, fp_hdi_upper, \n",
        "        alpha=0.5, color='lightgreen', lw=0)\n",
        "    \n",
        "    # Plot inducing points\n",
        "    if 'u' in particles.keys():\n",
        "        up_mean, up_hdi_lower, up_hdi_upper = particle_avg_hdi(particles['u'])\n",
        "        zp_mean, zp_hdi_lower, zp_hdi_upper = particle_avg_hdi(particles['Z'])\n",
        "        plt.plot(\n",
        "            zp_mean, up_mean,\n",
        "            'x', \n",
        "            label='posterior\\ninducing variables', \n",
        "            color='red',\n",
        "            alpha=1.0,\n",
        "            zorder=2)\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.title(title, fontsize=15)\n",
        "    plt.show()\n",
        "\n",
        "def plot_smc_posterior_particels(particles, idxs=None, sub_title=''):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # observations and true function\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5, zorder=-1)\n",
        "\n",
        "    # f particles\n",
        "    if idxs == None:\n",
        "        idxs = range(particles['f'].shape[0])\n",
        "    for i in idxs:\n",
        "        plt.plot(\n",
        "            x, particles['f'][i], \n",
        "            label='posterior f' if i==0 else '', \n",
        "            color='green', lw=1.5, zorder=1, alpha=0.6)\n",
        "    \n",
        "        # Plot inducing points\n",
        "        # up_mean, up_hdi_lower, up_hdi_upper = particle_avg_hdi(particles['u'])\n",
        "        # zp_mean, zp_hdi_lower, zp_hdi_upper = particle_avg_hdi(particles['Z'])\n",
        "        plt.plot(\n",
        "            particles['Z'][i], particles['u'][i],\n",
        "            'x', \n",
        "            label='posterior\\ninducing variables', \n",
        "            color='red',\n",
        "            alpha=1.0,\n",
        "            zorder=2)\n",
        "    \n",
        "    plt.legend()\n",
        "    if sub_title != '':\n",
        "        sub_title = f'\\n{sub_title}'\n",
        "    plt.title(f'Posterior estimate of Sparse GP{sub_title}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "num_particles = sampling_parameters['num_particles']\n",
        "num_upates = sampling_parameters['num_mcmc_steps']\n",
        "sub_title = f'\\n{num_particles} particles, {num_upates} updates, {num_inducing_points} inducing points'\n",
        "sub_title = sub_title + '\\nfree params: cov, u, and obs_noise' \n",
        "\n",
        "\n",
        "plot_smc_posterior(\n",
        "    particles, \n",
        "    title=f'Posterior estimate of Sparse GP' + sub_title)\n",
        "plot_smc_posterior(\n",
        "    lgp_particles, \n",
        "    title=f'Posterior estimate of full latent GP' + sub_title.split('\\nset')[0])\n",
        "\n",
        "plot_smc_prior( \n",
        "    initial_particles, \n",
        "    title=f'Prior of Sparse GP' + sub_title)\n",
        "plot_smc_prior(\n",
        "    lgp_inital, \n",
        "    title=f'Prior of full latent GP' + sub_title.split('\\nset')[0])\n",
        "\n",
        "\n",
        "# for i in range(10, 20):\n",
        "#     plot_smc_posterior_particels(\n",
        "#         particles,\n",
        "#         idxs=[i],\n",
        "#         sub_title=sub_title)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot parameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "QWqkOFsdLG9m",
        "outputId": "27a42599-0a4a-4b97-a4e9-062ebc2b2690"
      },
      "outputs": [],
      "source": [
        "def plot_parameter_histograms(particles, title='', prior_values=None):\n",
        "\n",
        "    params = list(particles.keys())\n",
        "\n",
        "    _, axs = plt.subplots(\n",
        "        nrows=1, ncols=len(params), \n",
        "        constrained_layout=True,\n",
        "        sharex='col', sharey='col', figsize=(12, 4))\n",
        "\n",
        "    for i, p in enumerate(params):\n",
        "        \n",
        "        ax = axs if type(axs) == plt.Axes else axs[i]\n",
        "\n",
        "        pd = particles[p]\n",
        "        # There are some outliers that skew the axis\n",
        "        pd_u, pd_l = jnp.percentile(pd, q=99.9), jnp.percentile(pd, q=0.1)\n",
        "        pd_filtered = jnp.extract(pd>pd_l, pd)\n",
        "        pd_filtered = jnp.extract(pd_filtered<pd_u, pd_filtered)\n",
        "        if pd_filtered.shape[0] == 0:\n",
        "            pd_filtered = pd\n",
        "\n",
        "        ax.hist(pd_filtered, bins=30, density=True, color='tab:blue')\n",
        "        ax.set_xlabel(p)\n",
        "        \n",
        "        if p in ground_truth.keys():\n",
        "            ax.axvline(\n",
        "                x=ground_truth[p], \n",
        "                ls='--', lw=1.5, c='red', \n",
        "                label=f'true ({ground_truth[p]})')\n",
        "        \n",
        "        mean = jnp.mean(pd_filtered)\n",
        "        ax.axvline(\n",
        "            x=mean, \n",
        "            ls='--', c='black', \n",
        "            label=f'sample\\nmean ({jnp.round(mean, decimals=3)})')\n",
        "        \n",
        "        if prior_values:\n",
        "            ax.axvline(\n",
        "            x=prior_values[i], \n",
        "            ls='--', c='cornflowerblue',\n",
        "            \n",
        "            label=f'prior mean')\n",
        "                \n",
        "        ax.legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "print('POSTERIOR')\n",
        "plot_parameter_histograms(\n",
        "   particles=particles.get('kernel', {}),\n",
        "   title='Posterior particles sparse GP',\n",
        "   prior_values=[prior_lengthscale_mean, prior_variance_mean])\n",
        "plot_parameter_histograms(\n",
        "   particles=lgp_particles.get('kernel', {}),\n",
        "   title='Posterior particles latent GP')\n",
        "\n",
        "plot_parameter_histograms(\n",
        "   particles=particles.get('likelihood', {}),\n",
        "   title='Posterior particles sparse GP',\n",
        "   prior_values=[prior_obs_noise_mean])\n",
        "plot_parameter_histograms(\n",
        "   particles=lgp_particles.get('likelihood', {}),\n",
        "   title='Posterior particles latent GP')\n",
        "\n",
        "\n",
        "print('PRIORS')\n",
        "plot_parameter_histograms(\n",
        "    particles=initial_particles.position.get('kernel', {}), \n",
        "    title='Prior particle samples')\n",
        "plot_parameter_histograms(\n",
        "    particles=initial_particles.position.get('likelihood', {}), \n",
        "    title='Prior particle samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot predictive f and u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test new predictive function\n",
        "_, key_pred = jrnd.split(key)\n",
        "    \n",
        "f_pred = gp_sparse.predict_f_from_u(key_pred, jnp.linspace(-1.25, 1.25, num=150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "iNmqrrphJ-tX",
        "outputId": "66c13219-ebac-4a59-937b-85a6f4d86728"
      },
      "outputs": [],
      "source": [
        "def plot_predictive_f(key, predictive_fn, x_pred, title):\n",
        "    _, key_pred = jrnd.split(key)\n",
        "    \n",
        "    f_pred = predictive_fn(key_pred, x_pred)\n",
        "\n",
        "    # setup plotting\n",
        "    fig, axes = plt.subplots(\n",
        "        nrows=2, ncols=1, figsize=(12, 7), sharex=True,\n",
        "        sharey=True, constrained_layout=True)\n",
        "\n",
        "    # plot each particle\n",
        "    ax = axes[0]\n",
        "    for i in jnp.arange(0, num_particles, step=10):\n",
        "        ax.plot(\n",
        "            x_pred, f_pred[i, :], \n",
        "            alpha=0.1, color='tab:blue', zorder=2)\n",
        "\n",
        "    # mean and HDI over particles\n",
        "    ax = axes[1]\n",
        "    f_mean = jnp.mean(f_pred, axis=0)\n",
        "    f_hdi_lower = jnp.percentile(f_pred, q=2.5, axis=0)\n",
        "    f_hdi_upper = jnp.percentile(f_pred, q=97.5, axis=0)\n",
        "    ax.plot(\n",
        "        x_pred, f_mean, \n",
        "        color='tab:blue', lw=2, zorder=2, alpha=0.3,\n",
        "        label='f mean')\n",
        "    ax.fill_between(\n",
        "        x_pred, f_hdi_lower, f_hdi_upper,\n",
        "        alpha=0.2, color='tab:blue', lw=0)\n",
        "\n",
        "    # True f, observations and others for all axis\n",
        "    for ax in axes.flatten():\n",
        "        ax.plot(x, f_true, 'k', label=r'$f$', zorder=-1, alpha=0.5)\n",
        "        ax.plot(x, y, 'x', label='obs', color='black', alpha=0.5)\n",
        "        # ax.set_xlim([-10, 10])\n",
        "        ax.set_ylim([-5., 5.])\n",
        "        ax.set_xlabel(r'$x$')\n",
        "        ax.legend()\n",
        "\n",
        "    axes[0].set_title('SMC particles', fontsize=12)\n",
        "    axes[1].set_title('Posterior 95% HDI', fontsize=12)\n",
        "    fig.suptitle(title, fontsize=15)\n",
        "    #axes[0].set_ylabel('Latent GP', rotation=0, ha='right');\n",
        "\n",
        "plot_predictive_f(\n",
        "    key,\n",
        "    predictive_fn=gp_sparse.predict_f,\n",
        "    x_pred=jnp.linspace(-1.25, 1.25, num=150),\n",
        "    title='Predictive f sparse GP'\n",
        ")\n",
        "\n",
        "plot_predictive_f(\n",
        "    key,\n",
        "    predictive_fn=gp_sparse.predict_f_from_u,\n",
        "    x_pred=jnp.linspace(-1.25, 1.25, num=150),\n",
        "    title='Predictive f from u sparse GP'\n",
        ")\n",
        "\n",
        "plot_predictive_f(\n",
        "    key,\n",
        "    gp_latent.predict_f,\n",
        "    x_pred=jnp.linspace(-1.25, 1.25, num=150),\n",
        "    title='Predictive f latent GP'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### diagnostics _compute_sparse_gp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the code one can test if the compute sparse gp works correctly.\n",
        "\n",
        "Varying the number of evenly spaced inducing points shows how many points are possible to approximate the true function well. About 16 is works really well in the current setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gp_diagnostic = SparseGPModel(\n",
        "    x, y, \n",
        "    cov_fn=jk.RBF(), priors=priors, \n",
        "    num_inducing_points=10,\n",
        "    f_true=f_true)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calling compute_sparse_gp through init_fn\n",
        "# NOTE: The endresult is dependent on the prior.\n",
        "\n",
        "random_random_seed = np.random.randint(0, 10000 + 1)\n",
        "key = jrnd.PRNGKey(random_random_seed)\n",
        "key, key_prior = jrnd.split(key)\n",
        "\n",
        "state = gp_diagnostic.init_fn(key_prior, 1)\n",
        "print(state.position.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "plt.plot(\n",
        "    x, state.position['f'],\n",
        "    label=r'prior $f$', \n",
        "    color='lightblue',\n",
        "    alpha=0.8, lw=2, zorder=1)\n",
        "plt.plot(\n",
        "    state.position['Z'], state.position['u'], \n",
        "    'x', \n",
        "    label='prior\\ninducing points', \n",
        "    color='cornflowerblue', \n",
        "    alpha=1, lw=1, zorder=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calling compute_sparse_gp directly \n",
        "\n",
        "cov_params = {\n",
        "    'lengthscale': jnp.array(lengthscale_),\n",
        "    'variance': jnp.array(output_scale_)}\n",
        "ic(cov_params)\n",
        "\n",
        "mean_gp, cov_gp = gp_diagnostic._compute_sparse_gp(\n",
        "    cov_params=cov_params, \n",
        "    x=x,\n",
        "    samples_Z=x, \n",
        "    samples_u=f_true, \n",
        "    add_jitter=True)\n",
        "plt.imshow(cov_gp)\n",
        "plt.show()\n",
        "\n",
        "# Sample from GP\n",
        "random_random_seed = np.random.randint(0, 10000 + 1)\n",
        "key = jrnd.PRNGKey(random_random_seed)\n",
        "print(random_random_seed)\n",
        "# key, key_sampling = jrnd.split(key)\n",
        "\n",
        "L = jnp.linalg.cholesky(cov_gp)\n",
        "z = jrnd.normal(key, shape=[n])\n",
        "samples_f = jnp.dot(L, z) + mean_gp\n",
        "\n",
        "plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "plt.plot(\n",
        "    x, \n",
        "    samples_f, #f_prior_mean, \n",
        "    label=r'prior $f$', \n",
        "    color='lightblue',\n",
        "    alpha=0.8,\n",
        "    lw=2,\n",
        "    zorder=1)\n",
        "plt.plot(\n",
        "    state.position['Z'], state.position['u'], \n",
        "    'x', \n",
        "    label='prior\\ninducing points', \n",
        "    color='cornflowerblue', \n",
        "    alpha=1, lw=1, zorder=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
