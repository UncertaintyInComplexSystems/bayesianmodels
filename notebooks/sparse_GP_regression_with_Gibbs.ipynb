{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96SWkc20ciX8"
      },
      "source": [
        "# Sparse GP regression with Gibbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKwBWZw9cj1T"
      },
      "outputs": [],
      "source": [
        "# !pip install git+ssh://git@github.com/UncertaintyInComplexSystems/bayesianmodels.git\n",
        "# !pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8JrgHPAFZff"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGK3SkpzntO1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use('Solarize_Light2')\n",
        "\n",
        "# os.environ['JAX_ENABLE_X64'] = 'True'\n",
        "import jax\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)  # crucial for Gaussian processes\n",
        "import jax.random as jrnd\n",
        "import jax.numpy as jnp\n",
        "import distrax as dx\n",
        "import jaxkern as jk\n",
        "\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "from uicsmodels.gaussianprocesses.sparsegp import SparseGPModel\n",
        "\n",
        "# confirm precision setting\n",
        "x = jrnd.uniform(jrnd.PRNGKey(0), (1000,), dtype=jnp.float64)\n",
        "x.dtype # --> dtype('float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cov(cov, title, iter='', show_negative=True, figsize=(4, 12)):    \n",
        "    # transform jax array to numpy array to allow mutation\n",
        "    cov_np = np.array(cov)  \n",
        "\n",
        "    cmap = plt.cm.plasma\n",
        "\n",
        "    # replace negativ values with nan to make them visible in the plot\n",
        "    if show_negative:\n",
        "        cmap.set_bad((.5, 0, 0, 1))\n",
        "        for c in np.argwhere(cov_np < 0):\n",
        "            cov_np[c[0], c[1]] = np.nan\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize) \n",
        "    image = plt.imshow(cov_np, cmap=cmap) \n",
        "    # creating new axes on the right side of (ax) for the colorbar\n",
        "    # this new axis has the same hight as the original axis\n",
        "    divider = make_axes_locatable(ax) \n",
        "    colorbar_axes = divider.append_axes(\"right\", \n",
        "                                        size=\"10%\", \n",
        "                                        pad=0.1)\n",
        "    plt.colorbar(image, \n",
        "                 cax=colorbar_axes, \n",
        "                 ticks=None if show_negative else [np.min(cov_np), np.mean(cov_np), np.max(cov_np)])\n",
        "\n",
        "    # colorbar_axes.ticklabel_format(style='plain')\n",
        "    ax.set_title(title)\n",
        "    plt.savefig(f'results/cov_{title}_iter{iter}.png')\n",
        "    plt.savefig(f'results/cov_{title}.png')\n",
        "    # plt.show()\n",
        "    plt.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAG6wB7bFZfh"
      },
      "source": [
        "## simulate data\n",
        "Simulate some data from a known GP so we can look at the inference of the hyperparameters):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "AVM9NuejlN99",
        "outputId": "a54c1c6b-d39a-4919-a506-222567605d8f"
      },
      "outputs": [],
      "source": [
        "random_random_seed = np.random.randint(0, 10000 + 1)\n",
        "print('seed:', random_random_seed)\n",
        "key = jrnd.PRNGKey(5609)  # 1106, 5368, 8928\n",
        "\n",
        "lengthscale_ = 1.0 #0.2\n",
        "output_scale_ = 5.0\n",
        "obs_noise_ = 0.7\n",
        "n = 100\n",
        "x = jnp.linspace(0, 15, n)[:, jnp.newaxis]  # NOTE: Rossie seem to use x-domain of [-1, 1]\n",
        "\n",
        "x_mean = jnp.mean(x)\n",
        "x_std = jnp.std(x)\n",
        "print(f'x: mean {jnp.round(x_mean, decimals=10)}, std {x_std}')\n",
        "\n",
        "kernel = jk.RBF()\n",
        "K = kernel.cross_covariance(params=dict(lengthscale=lengthscale_,\n",
        "                                        variance=output_scale_),\n",
        "                            x=x, y=x) + 1e-6*jnp.eye(n)\n",
        "# plot_cov(K, 'true cov')\n",
        "\n",
        "L = jnp.linalg.cholesky(K)\n",
        "z = jrnd.normal(key, shape=(n,))\n",
        "f_true = jnp.dot(L, z) + jnp.zeros_like(z)  # NOTE: True GP had mean=1\n",
        "\n",
        "key, obs_key = jrnd.split(key)\n",
        "y = f_true + obs_noise_*jrnd.normal(obs_key, shape=(n,))\n",
        "\n",
        "ground_truth = dict(f=f_true,\n",
        "                    lengthscale=lengthscale_,\n",
        "                    variance=output_scale_,\n",
        "                    obs_noise=obs_noise_)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(x, f_true, 'k', label=r'$f$')\n",
        "plt.plot(x, y, 'rx', label='obs')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "# plt.xlim([0., 1.])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WIP implementing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCzt_6wPFZfi"
      },
      "source": [
        "Set up the GP models, either with $\\mathbf{f}$ sampled explicitly, or with $\\mathbf{f}$ margnalized out (theobvious choice for a Gaussian likelihood, but sampling is shown for pedagogical reasons):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prior & init. model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prior over covariance hyperparameters, likelihood, and inducing inputs.\n",
        "\n",
        "num_inducing_points = 20\n",
        "\n",
        "priors = dict(\n",
        "    kernel=dict(  # Rossi uses lognormal priors over kernel parameters. \n",
        "        # NOTE: Rossi uses a seperate lengthscale per feature.\n",
        "        #   - do they mean locations with features? Or is this about multiple inputs?\n",
        "\n",
        "        # lengthscale = dx.Transformed( \n",
        "        #     dx.Normal(loc=0, scale=.5), # NOTE: Adjusted to be closer to real value \n",
        "        #     tfb.Exp()), \n",
        "        # lengthscale = dx.Normal(loc=lengthscale_, scale=0.3),  # HACk: almost true cov prior\n",
        "        lengthscale = dx.Deterministic(loc=lengthscale_),  # HACK: true cov\n",
        "\n",
        "        # variance = dx.Transformed( \n",
        "        #     dx.Normal(loc=1.5, scale=.5),  # NOTE: Adjusted to be closer to real value \n",
        "        #     tfb.Exp())), # log Normal as in Rossi\n",
        "        # variance = dx.Normal(loc=output_scale_, scale=1.0)),  # HACk: almost true cov prior\n",
        "        variance = dx.Deterministic(loc=output_scale_)),  # HACK: true cov\n",
        "\n",
        "    likelihood=dict(  # actually not modeled in Rossi\n",
        "        # obs_noise = dx.Transformed(\n",
        "        #    dx.Normal(loc=0., scale=1.), \n",
        "        #    tfb.Exp())\n",
        "        obs_noise = dx.Deterministic(loc=obs_noise_)  # HACK: Using true obs_noise without updating it. \n",
        "        ),\n",
        "\n",
        "    inducing_inputs_Z=dict( \n",
        "    # NOTE: Defining distribution over Z directly results in an error\n",
        "        # the __init__ tries to sample from each distribution in the prior.\n",
        "        mean=dx.Deterministic(\n",
        "            loc=jnp.ones(shape=num_inducing_points) * x_mean),\n",
        "        scale=dx.Deterministic(\n",
        "            loc=jnp.ones(shape=num_inducing_points) * x_std*.8)))\n",
        "\n",
        "gp_sparse = SparseGPModel(\n",
        "    x, y, \n",
        "    cov_fn=jk.RBF(), priors=priors, \n",
        "    num_inducing_points=num_inducing_points,\n",
        "    f_true=f_true)  \n",
        "# NOTE Defaults to likelihood=Gaussian()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibbs update (for-loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### first gibbs_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%script false --no-raise-error  # hack to skip execution of a cell\n",
        "\n",
        "# playground, for testing only\n",
        "gibbs_state = gp_sparse.init_fn(\n",
        "    key=key, \n",
        "    num_particles = sampling_parameters['num_particles'])\n",
        "\n",
        "def plot_prior():\n",
        "    # plot true function, inducing points, estimated function\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5)\n",
        "    plt.plot(x, gibbs_state.position['f'], label=r'prior $f$', color='lightblue')\n",
        "    plt.plot(\n",
        "        gibbs_state.position['Z'], \n",
        "        gibbs_state.position['u'], \n",
        "        'x', \n",
        "        label='inducing points', \n",
        "        color='red',\n",
        "        alpha=1.)\n",
        "    plt.xlabel('x & Z')\n",
        "    plt.ylabel('y & u')\n",
        "    # plt.xlim([0., 1.])\n",
        "    plt.legend()\n",
        "    plt.savefig(f'results/prior.png')\n",
        "    plt.show()\n",
        "\n",
        "plot_prior()\n",
        "\n",
        "# plot covariance matrices\n",
        "def _plot_covs():\n",
        "    for t in gibbs_state.position['dev'].keys():\n",
        "        c = gibbs_state.position['dev'][t]\n",
        "        print(f'{t}: {c.shape}')\n",
        "        if t == 'cov_gp':\n",
        "            plot_cov(c, t, show_negative=False, figsize=(8, 12))\n",
        "            plot_cov(c, t + '\\nred = negative values', \n",
        "                    show_negative=True, \n",
        "                    figsize=(8, 12))\n",
        "        else:\n",
        "            plot_cov(c, t, show_negative=False)\n",
        "\n",
        "# _plot_covs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# utils: pretty print gibbs state, IPython progress bar\n",
        "\n",
        "def update_progress(progress, extra=''):\n",
        "    bar_length = 50\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "\n",
        "    block = int(round(bar_length * progress))\n",
        "    clear_output(wait = True)\n",
        "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
        "    print(text, extra)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "%%script false --no-raise-error  # hack to skip execution of a cell\n",
        "\n",
        "gibbs_state_hist = [gibbs_state]\n",
        "for i in range(1, sampling_parameters['num_mcmc_steps']):\n",
        "    t_start = time.time()\n",
        "\n",
        "    key, key_gibbs = jrnd.split(key)  # TODO: Should I reuse the same key every iteration instead of splitting a new one?\n",
        "    gibbs_state_, _ = gp_sparse.gibbs_fn(key_gibbs, gibbs_state_hist[-1], sampling_parameters)\n",
        "    gibbs_state_hist.append(gibbs_state_)\n",
        "    \n",
        "    # dumb current gibbst state history\n",
        "    if i % 100 == 0:\n",
        "        file = open(f'results/gibbs_state_hist.pkl', 'wb')\n",
        "        pickle.dump(gibbs_state_hist, file)\n",
        "        file.close()\n",
        "\n",
        "    # update progressbar\n",
        "    if i % 10 == 0:\n",
        "        update_progress(i / sampling_parameters[\"num_mcmc_steps\"])\n",
        "\n",
        "    # show and save plots\n",
        "    if (i % 50 == 0) and (i > 0):\n",
        "        plot_posterior(gibbs_state_hist, num_of_samples=1_000, iter=i)\n",
        "        plot_inducing_variables(gibbs_state_hist)\n",
        "        plot_training_history(gibbs_state_hist)\n",
        "        # plot_cov(gibbs_state_hist[-1].position['dev']['cov_gp'], title='gp', iter=i)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(f'    iter {i}, duration: {jnp.round(time.time() - t_start, decimals = 3)} sec')\n",
        "\n",
        "update_progress(1)\n",
        "plot_posterior(\n",
        "    gibbs_state_hist,   \n",
        "    num_of_samples=1_000, \n",
        "    iter=sampling_parameters['num_mcmc_steps'])\n",
        "plot_inducing_variables(gibbs_state_hist)\n",
        "plot_training_history(gibbs_state_hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibbs update (Jax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_random_seed = np.random.randint(0, 10000 + 1)\n",
        "print('random random seed:', random_random_seed)\n",
        "\n",
        "# create new RNG for the model only to seperate data and model\n",
        "key = jrnd.PRNGKey(random_random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "sampling_parameters = dict(\n",
        "                           num_burn=0,\n",
        "                           num_samples=20_000  # num_mcmc_steps\n",
        "                           )\n",
        "# run inference\n",
        "with jax.disable_jit(disable=False):  # TODO: Add alarming print when disabled\n",
        "    key, gpl_key = jrnd.split(key)\n",
        "    initial_state, gibbs_states = gp_sparse.inference(\n",
        "        gpl_key, \n",
        "        mode='gibbs', \n",
        "        sampling_parameters=sampling_parameters)\n",
        "\n",
        " # save all data to file\n",
        "if True:\n",
        "    data = {\n",
        "        'x': x,\n",
        "        'f_true': f_true,\n",
        "        'initial_state': initial_state,\n",
        "        'gibbs_states': gibbs_states\n",
        "    }\n",
        "    file = open(f'results/gibbs_states.pkl', 'wb')\n",
        "    pickle.dump(data, file)\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nGibbs-state keys:')\n",
        "# print all keys and len/shape of corresponding data\n",
        "for k in gibbs_states.position.keys():\n",
        "    try:\n",
        "        print('    ', k, \n",
        "              gibbs_states.position[k]\n",
        "              if len(gibbs_states.position[k].shape) == 0 \n",
        "              else gibbs_states.position[k].shape, \n",
        "              end='\\n')\n",
        "    except:\n",
        "        print('    ', k, len(gibbs_states.position[k]), end='\\n')\n",
        "print()\n",
        "\n",
        "print('\\ninital values')\n",
        "print('lengthscale:', initial_state.position['lengthscale'])\n",
        "print('variance', initial_state.position['variance'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Plot predictive f\n",
        "# key, predict_f_key = jrnd.split(key)\n",
        "\n",
        "# f_samples = gp_sparse.predict_f(\n",
        "#     key=predict_f_key, \n",
        "#     x_pred=jnp.linspace(0, 15, 80))\n",
        "# print(f_samples.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_prior(initial_state):\n",
        "    \"\"\"\n",
        "    Plot true function and prior\n",
        "    \n",
        "    num_of_states - number of samples starting from the back of the chain to use for plotting the posterior. \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5, zorder=-1)\n",
        "\n",
        "    # prior\n",
        "    plt.plot(\n",
        "        x, \n",
        "        initial_state.position['f'], \n",
        "        label=r'prior $f$', \n",
        "        color='lightblue',\n",
        "        alpha=1,\n",
        "        lw=2,\n",
        "        zorder=1)\n",
        "    plt.plot(\n",
        "        initial_state.position['Z'], \n",
        "        initial_state.position['u'], \n",
        "        'x', \n",
        "        label='prior\\ninducing points', \n",
        "        color='cornflowerblue',\n",
        "        alpha=1,\n",
        "        lw=1,\n",
        "        zorder=1)\n",
        "    \n",
        "    plt.title(f'true function, noisy data, and prior')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_posterior(gibbs_state_hist, initial_state, num_of_samples=100):\n",
        "    \"\"\"\n",
        "    Plot true function and posterior samples\n",
        "    \n",
        "    num_of_states - number of samples starting from the back of the chain to use for plotting the posterior. \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(x, f_true, 'k', label=r'true $f$', lw=1.5, alpha=1, zorder=-1)\n",
        "    plt.plot(x, y, 'x', label='obs', color='black', alpha=0.5, zorder=-1)\n",
        "\n",
        "    plt.plot(\n",
        "        gibbs_state_hist.position['Z'][-1],\n",
        "        # jnp.mean(gibbs_state_hist.position['Z'][-num_of_samples: ], axis=0),\n",
        "        gibbs_state_hist.position['u'][-1], \n",
        "        # jnp.mean(gibbs_state_hist.position['u'][-num_of_samples: ], axis=0),\n",
        "        'x', \n",
        "        label='posterior\\ninducing points\\nlast sample', \n",
        "        color='red',\n",
        "        alpha=1.0,\n",
        "        zorder=2)\n",
        "\n",
        "    # plt.plot(\n",
        "    #     gibbs_state_hist.position['Z'][-num_of_samples: ],\n",
        "    #     gibbs_state_hist.position['u'][-num_of_samples: ], \n",
        "    #     'x', \n",
        "    #     # label='posterior\\ninducing points\\nall samples', \n",
        "    #     color='red',\n",
        "    #     alpha=0.1,\n",
        "    #     zorder=0)\n",
        "\n",
        "    plt.plot(\n",
        "        # gibbs_state_hist.position['Z'][-1],\n",
        "        jnp.mean(gibbs_state_hist.position['Z'][-num_of_samples: ], axis=0),\n",
        "        #gibbs_state_hist.position['u'][-1], \n",
        "        jnp.mean(gibbs_state_hist.position['u'][-num_of_samples: ], axis=0),\n",
        "        'x', \n",
        "        label='mean posterior\\ninducing points', \n",
        "        color='orange',\n",
        "        alpha=1.0,\n",
        "        zorder=2)\n",
        "\n",
        "    # prior\n",
        "    plt.plot(\n",
        "        x, \n",
        "        initial_state.position['f'], \n",
        "        label=r'prior $f$', \n",
        "        color='lightblue',\n",
        "        alpha=1,\n",
        "        lw=2,\n",
        "        zorder=1)\n",
        "    plt.plot(\n",
        "        initial_state.position['Z'], \n",
        "        initial_state.position['u'], \n",
        "        'x', \n",
        "        label='prior\\ninducing points', \n",
        "        color='cornflowerblue',\n",
        "        alpha=1,\n",
        "        lw=1,\n",
        "        zorder=1)\n",
        "\n",
        "    # get last `num_of_states` gibbs states\n",
        "    f_samples = gibbs_state_hist.position['f'][-1*num_of_samples:]\n",
        "\n",
        "    # plot each sample\n",
        "    '''for i in range(f_samples.shape[0]):  # negativ indexing to select last elements\n",
        "        plt.plot(x, f_samples[i], \n",
        "                color='orange',\n",
        "                alpha=0.1)'''\n",
        "        \n",
        "    # plot average and confidence intervals\n",
        "    f_mean = jnp.mean(f_samples, axis=0)\n",
        "    f_hdi_lower = jnp.percentile(f_samples, q=2.5, axis=0)\n",
        "    f_hdi_upper = jnp.percentile(f_samples, q=97.5, axis=0)\n",
        "    ax = plt.gca()\n",
        "    ax.plot(x, f_mean, label='mean f~GP', color='green', lw=1.5, zorder=1)\n",
        "    ax.fill_between(\n",
        "        x.flatten(), f_hdi_lower, f_hdi_upper, \n",
        "        alpha=0.5, color='lightgreen', lw=0)\n",
        "    # ax.set_ylim(\n",
        "    #     jnp.min(f_true) - 1, \n",
        "    #     jnp.max(f_true) + 1)\n",
        "    \n",
        "    plt.title(f'last {f_samples.shape[0]} posterior samples')\n",
        "    plt.legend()\n",
        "    # plt.savefig(f'results/posterior_iter{iter}.png')\n",
        "    # plt.savefig(f'results/posterior.png')\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "    \n",
        "def plot_cov_param_trajectory(lengthscale=None, variance=None, title=''):\n",
        "    # Plot training history\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax2 = ax.twinx()\n",
        "    cmap = plt.colormaps.get_cmap('Set1')\n",
        "\n",
        "    if lengthscale is not None:\n",
        "        ax.plot(\n",
        "            lengthscale, \n",
        "            label='lengthscale', \n",
        "            color=cmap(0), alpha=1,\n",
        "            zorder=1)\n",
        "        ax.axhline(\n",
        "            lengthscale_, \n",
        "            linestyle='--', \n",
        "            label=f'true lengthscale ({lengthscale_})',\n",
        "            color=cmap(0), alpha=1,\n",
        "            zorder=2)\n",
        "        ax.set_ylabel('lengthscale', color=cmap(0))\n",
        "        lengthscale_mean = jnp.mean(lengthscale)\n",
        "        lengthscale_hdi_lower = jnp.percentile(lengthscale, q=2.5, axis=0)\n",
        "        lengthscale_hdi_upper = jnp.percentile(lengthscale, q=97.5, axis=0)\n",
        "        # ax.set_ylim(lengthscale_hdi_lower*1.5, lengthscale_hdi_upper*1.5)\n",
        "        # if lengthscale.shape[0] >= 200:\n",
        "        #     ax.set_ylim(\n",
        "        #         jnp.min(lengthscale[400:])*1.5, \n",
        "        #         jnp.max(lengthscale[400:])*1.5)\n",
        "\n",
        "    if variance is not None:\n",
        "        ax2.plot(\n",
        "            variance, \n",
        "            label='variance', \n",
        "            color=cmap(1), alpha=1,\n",
        "            zorder=3)\n",
        "        ax2.axhline(output_scale_, \n",
        "                    linestyle='--', \n",
        "                    label=f'true variance ({output_scale_})',\n",
        "                    color=cmap(1), alpha=1,\n",
        "                    zorder=4)\n",
        "        ax2.set_ylabel('variance', color=cmap(1))\n",
        "\n",
        "    # create shared legend for both axis\n",
        "    lines, labels = ax.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax2.legend(lines + lines2, labels + labels2, loc=0)\n",
        "\n",
        "    ax.set_xlabel('Gibbs iterations')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "\n",
        "def plot_param_trajectory(\n",
        "        param_gibbs_hist:dict, \n",
        "        title:str='', \n",
        "        color_idxs:list=[0, 1]):\n",
        "    # Plot training history\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))    \n",
        "    cmap = plt.colormaps.get_cmap('Set1')\n",
        "\n",
        "    for i, (param, data) in enumerate(param_gibbs_hist.items()):\n",
        "\n",
        "        if i == 1:\n",
        "            ax = ax.twinx()\n",
        "        ax.plot(data, label=param, color=cmap(color_idxs[i]))\n",
        "        # ax.axhline(lengthscale_, linestyle='--', color=cmap(0), \n",
        "        #         label=f'true lengthscale ({lengthscale_})')\n",
        "        ax.set_ylabel(param, color=cmap(color_idxs[i]))\n",
        "        ax.legend()\n",
        "\n",
        "    # create shared legend for both axis\n",
        "    # lines, labels = ax.get_legend_handles_labels()\n",
        "    #lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    # ax2.legend(lines + lines2, labels + labels2, loc=0)\n",
        "\n",
        "    ax.set_xlabel('iterations')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "\n",
        "def plot_u_trajectories(gibbs_state_hist, gibbs_key:str='u'):\n",
        "    # plot u trajectories\n",
        "    u_samples = gibbs_state_hist.position[gibbs_key]\n",
        "\n",
        "    iter_idx = range(len(u_samples))\n",
        "    color = plt.cm.hsv(np.linspace(0, 1, u_samples.shape[1]))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    ax = plt.gca()\n",
        "    ax.set_facecolor('grey')\n",
        "    for i in range(u_samples.shape[1]):\n",
        "        plt.plot(iter_idx, u_samples[:, i], alpha=0.7, color=color[i], lw=1)\n",
        "    plt.xlabel('Gibbs iterations')\n",
        "    plt.title(f'{gibbs_key} samples')\n",
        "    # plt.savefig(f'results/train_hist_{gibbs_key}.png')\n",
        "    plt.show()\n",
        "    # plt.close()\n",
        "\n",
        "def plot_inducing_variables(gibbs_state_hist):\n",
        "    plot_u_trajectories(gibbs_state_hist, gibbs_key='u')\n",
        "    plot_u_trajectories(gibbs_state_hist, gibbs_key='Z')\n",
        "\n",
        "def plot_param_histogram(\n",
        "    param_samples:dict,\n",
        "    bins:int = 20,\n",
        "    title:str='', \n",
        "    color_idxs:list=[0, 1]):\n",
        "\n",
        "    fig, ax = plt.subplots(\n",
        "        ncols=len(param_samples), \n",
        "        figsize=(12, 4), \n",
        "        squeeze=False)\n",
        "    cmap = plt.colormaps.get_cmap('Set1')\n",
        "\n",
        "    for i, (param, data) in enumerate(param_samples.items()):\n",
        "        ax[0, i].hist(\n",
        "            data,\n",
        "            density=True,\n",
        "            bins=bins, \n",
        "            color=cmap(color_idxs[i]),)\n",
        "        ax[0, i].set_title(param)\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_prior(initial_state)\n",
        "plot_posterior(\n",
        "    gibbs_states,\n",
        "    initial_state,\n",
        "    num_of_samples=sampling_parameters['num_samples'])\n",
        "plot_param_histogram(\n",
        "    param_samples = {\n",
        "        'lengthscale': gibbs_states.position['lengthscale'],\n",
        "        'variance': gibbs_states.position['variance'],\n",
        "        'obs_noise': gibbs_states.position['obs_noise']\n",
        "        },\n",
        "    color_idxs=[0, 1, 2]\n",
        ")\n",
        "plot_inducing_variables(gibbs_states)\n",
        "plot_cov_param_trajectory(\n",
        "    lengthscale = gibbs_states.position['lengthscale'],\n",
        "    variance = gibbs_states.position['variance'])\n",
        "plot_cov_param_trajectory(\n",
        "    lengthscale = gibbs_states.position['lengthscale'])\n",
        "plot_cov_param_trajectory(\n",
        "    variance = gibbs_states.position['variance'])\n",
        "\n",
        "plot_cov_param_trajectory(\n",
        "    lengthscale = gibbs_states.position['lengthscale'][:100],\n",
        "    variance = gibbs_states.position['variance'][:100],\n",
        "    title='first 100 iterations')\n",
        "plot_param_trajectory(\n",
        "    param_gibbs_hist = {\n",
        "        'lengthscale': gibbs_states.position['lengthscale'][200:],\n",
        "        'variance': gibbs_states.position['variance'][200:],\n",
        "        },\n",
        "    title='after 200 iter',\n",
        "    color_idxs=[0, 1]\n",
        "    )\n",
        "plot_param_trajectory(\n",
        "    param_gibbs_hist = {'obs_noise': gibbs_states.position['obs_noise']},\n",
        "    color_idxs=[2]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A4Qz65kFZfk"
      },
      "source": [
        "---\n",
        "\n",
        "# hidden\n",
        "Run inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZIUP33-LDeD",
        "outputId": "f342335d-112a-4ebc-92ea-35af01c0339a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "key, gpl_key = jrnd.split(key)\n",
        "lgp_particles, _, lgp_marginal_likelihood = gp_latent.inference(gpl_key,\n",
        "                                                                mode='gibbs-in-smc',\n",
        "                                                                sampling_parameters=dict(num_particles=num_particles, num_mcmc_steps=num_mcmc_steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7hxLP0SFZfl"
      },
      "source": [
        "Plot posteriors of hyperparameters for both models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "QWqkOFsdLG9m",
        "outputId": "27a42599-0a4a-4b97-a4e9-062ebc2b2690"
      },
      "outputs": [],
      "source": [
        "trainables = list()\n",
        "for component, val in priors.items():\n",
        "    trainables.extend(list(val.keys()))\n",
        "\n",
        "\n",
        "num_params = len(trainables)\n",
        "show_samples = jnp.array([int(i) for i in num_particles*jnp.linspace(0, 1, num=500)])\n",
        "\n",
        "symbols = dict(lengthscale='\\ell',\n",
        "               obs_noise='\\sigma',\n",
        "               variance=r'\\tau')\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=num_params, constrained_layout=True,\n",
        "                       sharex='col', sharey='col', figsize=(12, 6))\n",
        "axes = np.expand_dims(axes, axis=0)  # quick fix as I removed a whole row of plots\n",
        "\n",
        "for m, particles in enumerate([lgp_particles]):\n",
        "    for j, var in enumerate(trainables):\n",
        "        ax = axes[m, j]\n",
        "        pd = particles.particles[var]\n",
        "        # There are some outliers that skew the axis\n",
        "        pd_u, pd_l = jnp.percentile(pd, q=99.9), jnp.percentile(pd, q=0.1)\n",
        "        pd_filtered = jnp.extract(pd>pd_l, pd)\n",
        "        pd_filtered = jnp.extract(pd_filtered<pd_u, pd_filtered)\n",
        "        ax.hist(pd_filtered, bins=30, density=True, color='tab:blue')\n",
        "        if var in symbols and m==1:\n",
        "            ax.set_xlabel(r'${:s}$'.format(symbols[var]))\n",
        "\n",
        "plt.suptitle(f'Posterior estimate of Bayesian GP ({num_particles} particles)');\n",
        "\n",
        "axes[0, 0].set_ylabel('Latent GP', rotation=0, ha='right')\n",
        "\n",
        "if len(ground_truth):\n",
        "    for j, var in enumerate(trainables):\n",
        "        axes[0, j].axvline(x=ground_truth[var], ls='--', c='k');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYHPvfgvFZfn"
      },
      "source": [
        "And plot the posterior predictive of $\\mathbf{f}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "iNmqrrphJ-tX",
        "outputId": "66c13219-ebac-4a59-937b-85a6f4d86728"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "x_pred = jnp.linspace(-0.25, 1.25, num=150)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharex=True,\n",
        "                            sharey=True, constrained_layout=True)\n",
        "axes = np.expand_dims(axes, axis=0)  # quick fix as I removed a whole row of plots\n",
        "\n",
        "for j, gp in enumerate([gp_latent]):\n",
        "    key, key_pred = jrnd.split(key)\n",
        "    f_pred = gp.predict_f(key_pred, x_pred)\n",
        "\n",
        "    ax = axes[j, 0]\n",
        "    for i in jnp.arange(0, num_particles, step=10):\n",
        "        ax.plot(x_pred, f_pred[i, :], alpha=0.1, color='tab:blue')\n",
        "\n",
        "    ax = axes[j, 1]\n",
        "    f_mean = jnp.mean(f_pred, axis=0)\n",
        "    f_hdi_lower = jnp.percentile(f_pred, q=2.5, axis=0)\n",
        "    f_hdi_upper = jnp.percentile(f_pred, q=97.5, axis=0)\n",
        "\n",
        "    ax.plot(x_pred, f_mean, color='tab:blue', lw=2)\n",
        "    ax.fill_between(x_pred, f_hdi_lower, f_hdi_upper,\n",
        "                    alpha=0.2, color='tab:blue', lw=0)\n",
        "\n",
        "for ax in axes.flatten():\n",
        "    ax.plot(x, f_true, 'k', label=r'$f$')\n",
        "    ax.plot(x, y, 'rx', label='obs')\n",
        "    ax.set_xlim([-0.25, 1.25])\n",
        "    ax.set_ylim([-4., 5.])\n",
        "    ax.set_xlabel(r'$x$')\n",
        "\n",
        "axes[0, 0].set_title('SMC particles')\n",
        "axes[0, 1].set_title('Posterior 95% HDI')\n",
        "\n",
        "axes[0, 0].set_ylabel('Latent GP', rotation=0, ha='right');"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
