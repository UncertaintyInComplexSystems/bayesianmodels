from uicsmodels.bayesianmodels import BayesianModel, GibbsState, ArrayTree
from uicsmodels.sampling.inference import update_correlated_gaussian, update_metropolis
from uicsmodels.gaussianprocesses.meanfunctions import Zero
from uicsmodels.gaussianprocesses.likelihoods import AbstractLikelihood, Gaussian
from uicsmodels.gaussianprocesses.fullgp import FullGPModel

from jax import Array
from jaxtyping import Float
from jax.random import PRNGKeyArray as PRNGKey
from typing import Callable, Union, Dict, Any, Optional, Iterable, Mapping

ArrayTree = Union[Array, Iterable["ArrayTree"], Mapping[Any, "ArrayTree"]]

import jax
import distrax as dx
import jax.numpy as jnp
from jax.random import PRNGKey
import jax.random as jrnd
from blackjax import elliptical_slice, rmh

import numpy as np

from tensorflow_probability.substrates import jax as tfp
tfd = tfp.distributions
tfb = tfp.bijectors

jitter = 1e-6


class SparseGPModel(FullGPModel):  # TODO: Switch to use BayesianModel

    """The latent Gaussian process model.
    
    The latent Gaussian process model consists of observations (y), generated by
    an observation model that takes the latent Gaussian process (f) and optional
    hyperparameters (phi) as input. The latent GP itself is parametrized by a
    mean function (mu) and a covariance function (cov). These can have optional
    hyperparameters (psi) and (theta).

    The generative model is given by:

    .. math::
        psi     &\sim p(\psi)\\
        theta   &\sim p(\theta) \\
        phi     &\sim p(\phi) \\
        f       &\sim GP(mu, cov) \\
        y       &\sim p(y \mid T(f), phi)

    """
    # NOTE: Update description
    # TODO: Change to factorize out f. 

    def __init__(self, X, y,
                 cov_fn: Optional[Callable],
                 mean_fn: Callable = None,
                 priors: Dict = None,
                 likelihood: AbstractLikelihood = None,
                 num_inducing_points: int = None):
        if likelihood is None:
            likelihood = Gaussian()
        self.likelihood = likelihood
        self.m = num_inducing_points  # TODO: Instead of passing, infer from prior over inducing inputs
        super().__init__(X, y, cov_fn, mean_fn, priors)        
        

    #
    def __get_component_parameters(self, position, component):
        """Extract parameter sampled values per model component for current
        position.

        """
        return {param: position[param] for param in
                self.param_priors[component]} if component in self.param_priors else {}

    #   
    def init_fn(self, key, num_particles=1):
        """Initialization of the Gibbs state.

        The initial state is determined by sampling all variables from their
        priors, and then constructing one sample for (f) using these. All
        variables together are stored in a dict() in the GibbsState object.

        When num_particles > 1, each dict element contains num_particles random
        initial values.

        Args:
            key:
                The jax.random.PRNGKey
            num_particles: int
                The number of parallel particles for sequential Monte Carlo
        Returns:
            GibbsState
        """
        print('# init_fn:')
        
        # Get all needed Random Keys 
        key, *sub_key = jrnd.split(key, num=3)
        key_super_init = sub_key[0]
        key_sample_latents = sub_key[1]

        initial_state = super().init_fn(key_super_init, num_particles)

        def sample_latent(key, initial_position_):
            # TODO: Split into seperate functions
            _, *sub_key = jrnd.split(key, num=4)
            key_sample_z = sub_key[0]
            key_sample_u = sub_key[1]
            key_sample_f = sub_key[2]

        # GP mean
            if 'mean_function' in self.param_priors.keys():
                mean_params = {param: initial_position_[param] for param in self.param_priors['mean_function']}  # TODO Q: Replace with calling `__get_component_parameters()` ?
                mean = self.mean_fn.mean(params=mean_params, x=self.X)
            else:
                # mean = jnp.zeros_like(self.X)
                mean = jnp.zeros((self.X.shape[0], ))
            if jnp.ndim(mean) == 1: 
                mean = mean[:, jnp.newaxis]

        # GP cov. 
            cov_params = None
            if 'kernel' in self.param_priors.keys():
                cov_params = {param: initial_position_[param] for param in self.param_priors['kernel']}
                cov_XX = self.kernel.cross_covariance(params=cov_params,
                                                   x=self.X,
                                                   y=self.X) + jitter * jnp.eye(self.n)
            else:
                cov_XX = jnp.eye(self.n)


            print('X', jnp.min(self.X), jnp.max(self.X))
            # HACK: Quick 'fix' to 'deal' with missing parameters when computing covariances of Z. 
            assert cov_params, 'cov_params not specified, will break sampling Z'


        # sample M inducing inputs Z
            Z_params = {param: initial_position_[param] for param in self.param_priors['inducing_inputs_Z']}
            print(f"params Z: {Z_params['mean'][0:3]}..., {Z_params['scale'][0:3]}...")

            # dist_Z = dx.Normal(Z_params['mean'], Z_params['scale'])
            # samples_Z = dist_Z.sample(seed=key_sample_z) # TODO: Sample with Cholesky instead.
            # samples_Z = jnp.sort(samples_Z)  # TODO: Q: Sorting necessary?
            # samples_Z = np.linspace(jnp.min(self.X), jnp.max(self.X), self.m) # : determinisitic Z for testing purposes
            # HACK: selecting evenly spaced subset of (X, y) using arange
            samples_Z_idx = jnp.arange(0, self.n, self.n//self.m)  # BUG: This does not result in exact number of inducing variables
            samples_Z = self.X[samples_Z_idx]
            if self.n%self.m != 0:
                print(f'Warning: can not obtain exact number {self.m} of inducing variables.')
            print(f"samples_Z: {samples_Z.shape} {samples_Z[0:3]}...")

        # compute cov. matrix ZZ 
            cov_ZZ = self.kernel.cross_covariance(
                params=cov_params,
                x=samples_Z,
                y=samples_Z) + jitter * jnp.eye(samples_Z.shape[0])
            # cov_ZZ_inv = jnp.linalg.inv(cov_ZZ) 
            np.linalg.cholesky(cov_ZZ)# HACK to check positive-definiteness

        # Sample inducing variables u
                # NOTE: To account for SMC particles, add dimension in `jrnd.normal`
            mean_u = jnp.zeros(self.m)
            print('mean_u', mean_u.shape)
            # samples_u = jnp.asarray(mean_u + jnp.dot(
            #    jnp.linalg.cholesky(cov_ZZ),
            #    jrnd.normal(key_sample_u, shape=[self.m]))) 
            samples_u = self.y[samples_Z_idx]
            print('samples_u:', samples_u.shape)


        # compute cov. between X and Z
            cov_XZ = self.kernel.cross_covariance(
                params=cov_params,
                x=self.X,
                y=samples_Z) # + jitter * jnp.eye(self.n, self.m)
            
        # compute cov. between Z and X
            cov_ZX = jnp.transpose(cov_XZ)

        # compute mean and cov. function of sparse GP
            # mean_gp = jnp.dot(jnp.dot(cov_XZ, cov_ZZ_inv), samples_u)
            mean_gp = jnp.dot(cov_XZ, jnp.linalg.solve(cov_ZZ, samples_u)) 
            print('mean_gp:', mean_gp.shape)

            # cov function
            # NOTE: make cov. diag of f. I might not actually need to if f is not integrated out
            # cov_gp = cov_XX - jnp.dot(jnp.dot(cov_XZ, jnp.linalg.inv(cov_ZZ)), cov_ZX)
            # ZZ_ZX = jax.scipy.linalg.cho_solve(jax.scipy.linalg.cho_factor(cov_ZZ), cov_ZX)
            ZZ_ZX = jnp.linalg.solve(cov_ZZ, cov_ZX)
            dot_XZ_ZZ_ZX = jnp.dot(cov_XZ, ZZ_ZX)
            cov_gp = cov_XX - jnp.dot(cov_XZ, ZZ_ZX)

            cov_gp += jitter * jnp.eye(self.n)
            # cov_gp = cov_gp * jnp.eye(self.n)  # keep only the diagnonal
            print('cov_gp', cov_gp.shape, cov_gp.dtype)
            tmp = cov_gp
            print('cov_gp min, avg, max', np.min(tmp), np.mean(tmp), np.max(tmp))
            np.linalg.cholesky(cov_gp)# HACK to check positive-definiteness

        # Sample from GP
                # NOTE: tell cholesky that the cov. is diagonal, if I set it too. 
                # NOTE: To account for SMC particles, add dimension in `jrnd.normal`
            L = jnp.linalg.cholesky(cov_gp)
            z = jrnd.normal(key_sample_f, shape=[self.n])
            samples_f = jnp.asarray(mean_gp + jnp.dot(L, z))
            print('samples_f', samples_f.shape, samples_f[0:5])

            dev = {'cov_XX':cov_XX,
                   'cov_XZ':cov_XZ, 
                   'cov_ZZ':cov_ZZ, 
                   'cov_ZX':cov_ZX, 
                   'ZZ_ZX' : ZZ_ZX,
                   'dot_XZ_ZZ_ZX' : dot_XZ_ZZ_ZX,
                   'cov_gp':cov_gp}
            return samples_Z.flatten(), samples_u, samples_f.flatten(), dev

        #
        initial_position = initial_state.position

        if num_particles > 1:
            # We vmap across the first dimension of the elements *in* the
            # dictionary, rather than over the dictionary itself.
            # DOLATER: u and Z won't be contained in gibbs_State if SMC is used.
            raise NotImplementedError
            initial_position['f'] = jax.vmap(
                sample_latent,
                in_axes = (0, {k: 0 for k in initial_position}))
            (key_sample_latents, initial_position)
        else:
            # HACK: Might be better to recieve a namedtuple from from `sample_latent`, or two tuples, one with names and the other with corresponding data
            samples_Z, samples_u, samples_f, dev = sample_latent(key_sample_latents, initial_position)
            initial_position['Z'] = samples_Z
            initial_position['u'] = samples_u
            initial_position['f'] = samples_f
            initial_position['dev'] = dev


        return GibbsState(initial_position)

        #


    def gibbs_fn(self, key, state, loglik_fn__=None, temperature=1.0, **mcmc_parameters): # HACK: Set loglik_fn to None
        """The Gibbs MCMC kernel.

        The Gibbs kernel step function takes a state and returns a new state. In
        the latent GP model, the latent GP (f) is first updated, then the
        parameters of the mean (psi) and covariance function (theta), and lastly
        the parameters of the observation model (phi).

        Args:
            key:
                The jax.random.PRNGKey
            state: GibbsState
                The current state in the MCMC sampler
        Returns:
            GibbsState

        """
        position = state.position.copy()

        """Sample the latent GP using:

        p(f | theta, psi, y) \propto p(y | f, phi) p(f | psi, theta)

        """
        likelihood_params = self.__get_component_parameters(position, 'likelihood')
        loglikelihood_fn_ = lambda f_: temperature * jnp.sum(self.likelihood.log_prob(params=likelihood_params, f=f_, y=self.y))

        mean_params = self.__get_component_parameters(position, 'mean')
        mean = self.mean_fn.mean(params=mean_params, x=self.X).flatten()

        cov_params = self.__get_component_parameters(position, 'kernel')
        cov = self.kernel.cross_covariance(params=cov_params,
                                           x=self.X, y=self.X) + jitter * jnp.eye(self.n)

        key, subkey = jrnd.split(key)
        position['f'], f_info = update_correlated_gaussian(subkey, position['f'], loglikelihood_fn_, mean, cov)

        if len(mean_params):  # NOTE None, Rossi uses zero mean
            """Sample parameters of the mean function using: 

            p(psi | f, theta) \propto p(f | psi, theta)p(psi)

            """

            def logdensity_fn_(psi_):
                log_pdf = 0
                for param, val in psi_.items():
                    log_pdf += jnp.sum(self.param_priors['mean'][param].log_prob(val))
                mean = self.mean_fn.mean(params=psi_, x=self.X).flatten()
                log_pdf += dx.MultivariateNormalFullCovariance(mean, cov).log_prob(position['f'])
                return log_pdf

            #
            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_metropolis(subkey, logdensity_fn_, mean_params, stepsize=mcmc_parameters.get('stepsizes', dict()).get('mean', 0.1))
            for param, val in sub_state.items():
                position[param] = val

            mean = self.mean_fn.mean(params=sub_state, x=self.X).flatten()
        #

        if len(cov_params):  # theta
            """Sample parameters of the kernel function using: 

            p(theta | f, psi) \propto p(f | psi, theta)p(theta)

            """

            def logdensity_fn_(theta_):
                log_pdf = 0
                for param, val in theta_.items():
                    log_pdf += jnp.sum(self.param_priors['kernel'][param].log_prob(val))
                cov_ = self.kernel.cross_covariance(params=theta_, x=self.X, y=self.X) + jitter * jnp.eye(self.n)
                log_pdf += dx.MultivariateNormalFullCovariance(mean, cov_).log_prob(position['f'])
                return log_pdf

            #
            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_metropolis(subkey, logdensity_fn_, cov_params, 
                                                    stepsize=mcmc_parameters.get('stepsizes', dict()).get('kernel', 0.1))
            for param, val in sub_state.items():
                position[param] = val
        #

        if len(likelihood_params):  # not specifed in Rossi
            """Sample parameters of the likelihood using: 

            p(\phi | y, f) \propto p(y | f, phi)p(phi)

            """

            def logdensity_fn_(phi_):
                log_pdf = 0
                for param, val in phi_.items():
                    log_pdf += jnp.sum(self.param_priors['likelihood'][param].log_prob(val))
                log_pdf += temperature*jnp.sum(self.likelihood.log_prob(params=phi_, f=position['f'], y=self.y))
                return log_pdf

            #
            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_metropolis(subkey, logdensity_fn_, likelihood_params, 
                                                    stepsize=mcmc_parameters.get('stepsizes', dict()).get('likelihood', 0.1))
            for param, val in sub_state.items():
                position[param] = val
        #

        return GibbsState(
            position=position), None  # We return None to satisfy SMC; this needs to be filled with acceptance information

    # 
    def loglikelihood_fn(self) -> Callable:
        """Returns the log-likelihood function for the model given a state.

        Args:
            None

        Returns:
            A function that computes the log-likelihood of the model given a
            state.
        """
        def loglikelihood_fn_(state: GibbsState) -> Float:
            # position = state.position
            position = getattr(state, 'position', state)
            phi = {param: position[param] for param in
                   self.param_priors['likelihood']} if 'likelihood' in self.param_priors else {}
            f = position['f']
            log_pdf = jnp.sum(self.likelihood.log_prob(params=phi, f=f, y=self.y))
            return log_pdf

        #
        return loglikelihood_fn_

    # 
    def logprior_fn(self) -> Callable:
        """Returns the log-prior function for the model given a state.

        Args:
            None
        Returns:
            A function that computes the log-prior of the model given a state.

        """

        def logprior_fn_(state: GibbsState) -> Float:
            position = getattr(state, 'position', state)  # to work in both Blackjax' MCMC and SMC environments
            logprob = 0
            for component, params in self.param_priors.items():
                # mean, kernel, likelihood
                for param, dist in params.items():
                    # parameters per component
                    logprob += jnp.sum(dist.log_prob(position[param]))
            # plus the logprob of the latent f itself
            psi = {param: position[param] for param in self.param_priors['mean']} if 'mean' in self.param_priors else {}
            theta = {param: position[param] for param in
                     self.param_priors['kernel']} if 'kernel' in self.param_priors else {}
            mean = self.mean_fn.mean(params=psi, x=self.X).flatten()
            cov = self.kernel.cross_covariance(params=theta,
                                               x=self.X,
                                               y=self.X) + jitter * jnp.eye(self.n)
            logprob += dx.MultivariateNormalFullCovariance(mean, cov).log_prob(position['f'])
            return logprob

        #
        return logprior_fn_

    #
    def predict_f(model, key: PRNGKey, x_pred: ArrayTree, num_subsample=-1):
        """Predict the latent f on unseen pointsand

        This function takes the approximated posterior (either by MCMC or SMC)
        and predicts new latent function evaluations f^*.

        Args:
            key: PRNGKey
            x_pred: x^*; the queried locations.
            num_subsample: By default, we return one predictive sample for each
            posterior sample. While accurate, this can be memory-intensive; this
            parameter can be used to thin the MC output to every n-th sample.

        Returns:
            f_samples: An array of samples of f^* from p(f^* | x^*, x, y)


        todo:
        - predict using either SMC or MCMC output
        - predict from prior if desired
        """

        @jax.jit
        def sample_predictive_f(key, x_pred: ArrayTree, **samples):
            """Sample latent f for new points x_pred given one posterior sample.

            See Rasmussen & Williams. We are sampling from the posterior predictive for
            the latent GP f, at this point not concerned with an observation model yet.

            We have [f, f*]^T ~ N(0, KK), where KK is a block matrix:

            KK = [[K(x, x), K(x, x*)], [K(x, x*)^T, K(x*, x*)]]

            This results in the conditional

            f* | x, x*, f ~ N(mu, cov), where

            mu = K(x*, x)K(x,x)^-1 f
            cov = K(x*, x*) - K(x*, x) K(x, x)^-1 K(x, x*)

            Args:
                key: The jrnd.PRNGKey object
                x_pred: The prediction locations x*
                state_variables: A sample from the posterior

            Returns:
                A single posterior predictive sample f*

            """

            def get_parameters_for(component):
                """Extract parameter sampled values per model component for current
                position.

                """
                return {param: samples[param] for param in
                        model.param_priors[component]} if component in model.param_priors else {}

            #

            f = samples['f']
            psi = get_parameters_for('mean')
            mean = model.mean_fn.mean(params=psi, x=x_pred).flatten()
            theta = get_parameters_for('kernel')

            Kxx = model.kernel.cross_covariance(params=theta, x=model.X, y=model.X)
            Kzx = model.kernel.cross_covariance(params=theta, x=x_pred, y=model.X)
            Kzz = model.kernel.cross_covariance(params=theta, x=x_pred, y=x_pred)

            Kxx += jitter * jnp.eye(*Kxx.shape)
            Kzx += jitter * jnp.eye(*Kzx.shape)
            Kzz += jitter * jnp.eye(*Kzz.shape)

            L = jnp.linalg.cholesky(Kxx)
            alpha = jnp.linalg.solve(L.T, jnp.linalg.solve(L, f))
            v = jnp.linalg.solve(L, Kzx.T)
            predictive_mean = mean + jnp.dot(Kzx, alpha)
            predictive_var = Kzz - jnp.dot(v.T, v)

            predictive_var += jitter * jnp.eye(*Kzz.shape)

            C = jnp.linalg.cholesky(predictive_var)
            z = jrnd.normal(key, shape=(len(x_pred),))

            f_samples = predictive_mean + jnp.dot(C, z)
            return f_samples

        #

        num_particles = model.particles.particles['f'].shape[0]
        key_samples = jrnd.split(key, num_particles)

        f_pred = jax.vmap(sample_predictive_f,
                            in_axes=(0, None))(key_samples, x_pred, 
                                            **model.particles.particles)
        return f_pred

    #
    def predict_y(self, key, x_pred):
        # todo; call predict_f first, then the dx random from the appropriate likelihood
        pass

    #
#